{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STANDARD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "import operator\n",
    "\n",
    "# SKLEARN\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# XGBOOST\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, DMatrix\n",
    "\n",
    "# RANDOM FOREST\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# KERAS\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensemble Builder (Partially Redacted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper to get a specific number of features\n",
    "def set_features(df, target_col, n_features):\n",
    "    \n",
    "    # get list of features and slice the n we need, then add back target\n",
    "    features_list = [col for col in df.columns if col != target_col][:n_features]\n",
    "    features_list.append(target_col)\n",
    "    \n",
    "    return features_list\n",
    "\n",
    "# reshape data so that LSTM doesn't get mad\n",
    "def reshape_data(array, time_steps):\n",
    "    \n",
    "    # set length / number of samples\n",
    "    L = array.shape[0] - time_steps + 1\n",
    "    \n",
    "    # use strides for something\n",
    "    strided = np.lib.stride_tricks.as_strided\n",
    "    m, n = array.strides\n",
    "    \n",
    "    # set width / number of features\n",
    "    N = array.shape[1]\n",
    "    \n",
    "    # and finally, reshape data according to specifications\n",
    "    reshaped = strided(array, shape = (L, time_steps, N), strides = (N * n, m, n))\n",
    "    \n",
    "    return reshaped.copy()\n",
    "\n",
    "# get X, Y, and column headers / names for random forest\n",
    "def get_lstm_matrix(df, target_col, periods_ahead, drop_target = False, rate_of_change = False):\n",
    "    \n",
    "    # read in all cols with closing prices\n",
    "    df = df[df[target_col].notnull()]\n",
    "    \n",
    "    # clean data for BTC_ETH starts on 2015-08-09\n",
    "    df = df[df.index >= datetime(2015, 8, 9)]\n",
    "    \n",
    "    # select target column and create it in dataset\n",
    "    target_col_name = target_col + '_target'\n",
    "    df[target_col_name] = df[target_col].shift(-periods_ahead)\n",
    "    \n",
    "    # if looking at the rate of change instead of actual values\n",
    "    if rate_of_change:\n",
    "        df[target_col_name] = (df[target_col_name] - df[target_col]) / df[target_col]\n",
    "    \n",
    "    # drop the original target col\n",
    "    if drop_target:\n",
    "        df = df.drop(target_col, 1)\n",
    "    \n",
    "    # drop null values\n",
    "    df.dropna(inplace = True)\n",
    "    \n",
    "    # convert to matrix\n",
    "    dat = df.as_matrix()\n",
    "    \n",
    "    # convert to float (just in case)\n",
    "    dat = dat.astype('float32')\n",
    "    \n",
    "    # get dates for QA\n",
    "    dates = df.index.tolist()\n",
    "    \n",
    "    return df, dat, dates\n",
    "\n",
    "# get lagged version of X (straight up, no RoC)\n",
    "def get_lagged_x_straight(_array, timesteps):\n",
    "    \n",
    "    # reshpae to add timesteps\n",
    "    reshaped_array = reshape_data(_array, timesteps)\n",
    "    \n",
    "    # flatten to two dimensions\n",
    "    flattened_array = np.array([list(sub_arr.reshape(1,-1)[0]) for sub_arr in reshaped_array])\n",
    "    \n",
    "    return flattened_array\n",
    "\n",
    "# get a lagged version of the dataset for a specific n of timesteps\n",
    "def get_lagged_dataset(df, target_col, periods_ahead, n_features, timesteps, roc):\n",
    "    \n",
    "    # filter dowm to n features needed\n",
    "    df = df[set_features(df, target_col, n_features)]\n",
    "    \n",
    "    # pre-process data\n",
    "    adf, dat, dates = get_lstm_matrix(df,\n",
    "                                      target_col = target_col,\n",
    "                                      periods_ahead = periods_ahead,\n",
    "                                      drop_target = True,\n",
    "                                      rate_of_change = True\n",
    "                                     )\n",
    "    \n",
    "    # add the target col\n",
    "    adf['direction'] = adf[target_col + '_target'].apply(lambda x: np.sign(x))\n",
    "\n",
    "    # reverse order of df so it is more intuitive\n",
    "    adf = adf.sort_index(ascending = False)\n",
    "\n",
    "    # set X and y\n",
    "    X = adf.ix[:,:-2].as_matrix().copy()\n",
    "    y = adf.ix[:,-1].as_matrix().copy()\n",
    "    \n",
    "    if roc:\n",
    "        # transform X to add n lag\n",
    "        X = get_lagged_x_roc(X, timesteps).copy()\n",
    "    elif not roc:\n",
    "        # transform X to add n lag\n",
    "        X = get_lagged_x_straight(X, timesteps).copy()\n",
    "        \n",
    "    else: 'Please choose lag type!'\n",
    "    \n",
    "    # trim y to match X\n",
    "    y = y[:-timesteps+1].copy()\n",
    "    \n",
    "    return X, y, adf\n",
    "\n",
    "# helper to return the primary KPIs of a regression model from a list of predicted and true y values\n",
    "def regression_kpis(prediction_list, scaler = None):\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for true, pred in prediction_list:\n",
    "        y_true.append(true)\n",
    "        y_pred.append(pred)\n",
    "    \n",
    "    # invert scale / predictions\n",
    "    if scaler != None:\n",
    "        y_true = scaler.inverse_transform(y_true)\n",
    "        y_pred = scaler.inverse_transform(y_pred)\n",
    "    \n",
    "    print('MAE:', mae(y_true, y_pred))\n",
    "    print('RMSE:', mse(y_true, y_pred) ** 0.5)\n",
    "    print('Sign Accuracy:', sum([np.sign(true) == np.sign(pred) for true, pred in zip(y_true, y_pred)]) / len(prediction_list))\n",
    "\n",
    "# run walk forward validation on a sklearn model (has to be fed in as model(), meaning params must be init then)\n",
    "def sklearn_wfv_regression(X, y, _model, walk_forward_window, verbose = 0):\n",
    "    \n",
    "    prediction_list = []\n",
    "\n",
    "    for wf in list(reversed(range(1, walk_forward_window + 1))):\n",
    "        \n",
    "        # get the test y\n",
    "        test_Y = y[-wf].copy()\n",
    "\n",
    "        # create training set using scaled data\n",
    "        train_X = X[:-wf,:].copy()\n",
    "        train_y = y[:-wf].copy()\n",
    "\n",
    "        # create, scale, and reshape test set\n",
    "        if wf > 1:\n",
    "            test_X = X[:-wf+1,:].copy()\n",
    "            test_Y_acc_check = y[:-wf+1].copy()\n",
    "        elif wf == 1:\n",
    "            test_X = X.copy()\n",
    "            test_Y_acc_check = y.copy()\n",
    "\n",
    "        # fit model no training data\n",
    "        model = _model\n",
    "        model.fit(train_X, train_y)\n",
    "\n",
    "        # make predictions for test data\n",
    "        y_pred = model.predict(test_X)\n",
    "\n",
    "        # get the last value of the predictions (the only on that is a test pred)\n",
    "        yhat = y_pred[-1]\n",
    "        \n",
    "        if verbose == 1:\n",
    "            print('\\n', wf - 1, 'steps left')\n",
    "        elif verbose == 2:\n",
    "            print('\\n', wf - 1, 'steps left')\n",
    "            print('predicted:', yhat)\n",
    "            print('actual:', test_Y, '\\n')\n",
    "\n",
    "        # create a target and sequence tuple of results for saving\n",
    "        tar_tuple = (test_Y, yhat)\n",
    "\n",
    "        prediction_list.append(tar_tuple)\n",
    "    \n",
    "    print('complete!')\n",
    "    return prediction_list\n",
    "\n",
    "# helper to duplicate the 24hr predictions to join the 6hr predictions\n",
    "def duplicate_24hr_predictions_for_6hr(df):\n",
    "    \n",
    "    time_suffix = [' 00:00:00', ' 06:00:00', ' 12:00:00', ' 18:00:00']\n",
    "    dup_24_df = pd.DataFrame()\n",
    "\n",
    "    for time in time_suffix:\n",
    "        merged_24hr['date'] = merged_24hr['Date'].apply(lambda x: x + time)\n",
    "        dup_24_df = dup_24_df.append(merged_24hr)\n",
    "    \n",
    "    dup_24_df = dup_24_df.drop('Date', 1)\n",
    "    dup_24_df.index = pd.to_datetime(dup_24_df.date)\n",
    "    dup_24_df = dup_24_df.drop('date', 1)\n",
    "    dup_24_df = dup_24_df.sort_index()\n",
    "    \n",
    "    return dup_24_df\n",
    "\n",
    "# helper to get ranked features using random forrest regression\n",
    "def get_ranked_features(X, Y, model_params, col_names, nb_epochs):\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    for i in range(nb_epochs):\n",
    "        \n",
    "        # create / fit new random forest model\n",
    "        rf = RandomForestRegressor(**model_params)\n",
    "        rf.fit(X, Y)\n",
    "        \n",
    "        # loop through each col and add up the scores\n",
    "        for score, col in zip(map(lambda x: x, rf.feature_importances_), col_names):\n",
    "            if i == 0:\n",
    "                scores[col] = score\n",
    "            else:\n",
    "                scores[col] += score\n",
    "    \n",
    "    # sort the scores in descending order\n",
    "    sorted_scores = sorted(scores.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    \n",
    "    return sorted_scores\n",
    "\n",
    "# helper to get top n features from ranked list\n",
    "def get_top_rf_features(scores, n):\n",
    "    top_features = [item[0] for item in scores[:n]]\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in training data\n",
    "df6 = pd.read_csv('_trainingData/6hr_newpair_train_filtered.csv')\n",
    "df24 = pd.read_csv('_trainingData/24hr_newpair_train_filtered.csv')\n",
    "\n",
    "# clean import\n",
    "df6.index = pd.to_datetime(df6.date)\n",
    "df24.index = pd.to_datetime(df24.date)\n",
    "df6 = df6.drop('date', 1)\n",
    "df24 = df24.drop('date', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usdteth_median_price</th>\n",
       "      <th>usdteth_median_price_t+4</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-06-19 18:00:00</th>\n",
       "      <td>354.96</td>\n",
       "      <td>346.00</td>\n",
       "      <td>-0.025242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-20 00:00:00</th>\n",
       "      <td>359.91</td>\n",
       "      <td>341.82</td>\n",
       "      <td>-0.050263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-20 06:00:00</th>\n",
       "      <td>360.80</td>\n",
       "      <td>330.09</td>\n",
       "      <td>-0.085116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-20 12:00:00</th>\n",
       "      <td>362.70</td>\n",
       "      <td>320.99</td>\n",
       "      <td>-0.114999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-20 18:00:00</th>\n",
       "      <td>346.00</td>\n",
       "      <td>304.29</td>\n",
       "      <td>-0.120549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     usdteth_median_price  usdteth_median_price_t+4    target\n",
       "date                                                                         \n",
       "2017-06-19 18:00:00                354.96                    346.00 -0.025242\n",
       "2017-06-20 00:00:00                359.91                    341.82 -0.050263\n",
       "2017-06-20 06:00:00                360.80                    330.09 -0.085116\n",
       "2017-06-20 12:00:00                362.70                    320.99 -0.114999\n",
       "2017-06-20 18:00:00                346.00                    304.29 -0.120549"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter 6hr down to the relevant cols\n",
    "df = pd.DataFrame()\n",
    "df['usdteth_median_price'] = df6['polo_usdteth_median_trade_price'].dropna()\n",
    "df['usdteth_median_price_t+4'] = df['usdteth_median_price'].shift(-4)\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "# calc target\n",
    "df['target'] = (df['usdteth_median_price_t+4'] - df['usdteth_median_price']) / df['usdteth_median_price']\n",
    "\n",
    "# 2017-06-19 18:00:00 marks the start of the LSTM 6hr predictions\n",
    "df = df[df.index >= datetime(2017, 6, 19, 18)]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Regression (6hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_6hr_preds = {}\n",
    "\n",
    "# save the predictions of each model into a dict for easy df\n",
    "for m in lstm_6hr_model_raw.keys():\n",
    "    m_preds = [pred[0][1] for pred in lstm_6hr_model_raw[m]]\n",
    "    lstm_6hr_preds[m] = m_preds\n",
    "\n",
    "# convert to df\n",
    "lstm_6hr_pred_df = pd.DataFrame(lstm_6hr_preds)\n",
    "\n",
    "# add dt index to the preds\n",
    "lstm_6hr_pred_df.index = df.index\n",
    "\n",
    "# and then join them together\n",
    "df = lstm_6hr_pred_df.join(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classification (6hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # predictions start 2017-06-11 18:00:00\n",
    "# xgb_6hr_results = pd.DataFrame(xgb_6hr_model_raw)\n",
    "\n",
    "# # split up predicted and actual values and write to csv for inspection\n",
    "# xgb_6hr_results_split = {}\n",
    "\n",
    "# for k in xgb_6hr_results:\n",
    "    \n",
    "#     true_list = []\n",
    "#     pred_list = []\n",
    "    \n",
    "#     for true, pred in xgb_6hr_results[k]:\n",
    "#         true_list.append(true)\n",
    "#         pred_list.append(pred)\n",
    "    \n",
    "#     xgb_6hr_results_split[k + '_ACTUAL'] = true_list\n",
    "#     xgb_6hr_results_split[k + '_PREDICTED'] = pred_list\n",
    "\n",
    "# # export to csv\n",
    "# pd.DataFrame(xgb_6hr_results_split).to_csv('xgb_6hr_temp_export.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6hrs_4steps_40features_1layers_300epochs_1timesteps_0.4dropout_52units</th>\n",
       "      <th>6hrs_4steps_40features_2layers_300epochs_1timesteps_0.1dropout_128units</th>\n",
       "      <th>6hrs_4steps_40features_2layers_300epochs_1timesteps_0.1dropout_96units</th>\n",
       "      <th>6hrs_4steps_40features_2layers_300epochs_1timesteps_0.2dropout_64units</th>\n",
       "      <th>6hrs_4steps_80features_1layers_300epochs_1timesteps_0.2dropout_52units</th>\n",
       "      <th>usdteth_median_price</th>\n",
       "      <th>usdteth_median_price_t+4</th>\n",
       "      <th>target</th>\n",
       "      <th>direction</th>\n",
       "      <th>xgb_6hrs_4steps_20features_1000estimators_20timesteps_6maxdepth_0.01learningrate</th>\n",
       "      <th>xgb_6hrs_4steps_20features_1000estimators_20timesteps_8maxdepth_0.01learningrate</th>\n",
       "      <th>xgb_6hrs_4steps_20features_1500estimators_20timesteps_6maxdepth_0.01learningrate</th>\n",
       "      <th>xgb_6hrs_4steps_20features_250estimators_24timesteps_8maxdepth_0.1learningrate</th>\n",
       "      <th>xgb_6hrs_4steps_20features_500estimators_20timesteps_8maxdepth_0.01learningrate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-06-19 18:00:00</th>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.034896</td>\n",
       "      <td>0.032555</td>\n",
       "      <td>0.018816</td>\n",
       "      <td>0.009536</td>\n",
       "      <td>354.96</td>\n",
       "      <td>346.00</td>\n",
       "      <td>-0.025242</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-20 00:00:00</th>\n",
       "      <td>-0.015428</td>\n",
       "      <td>-0.001259</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.006405</td>\n",
       "      <td>-0.013744</td>\n",
       "      <td>359.91</td>\n",
       "      <td>341.82</td>\n",
       "      <td>-0.050263</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-20 06:00:00</th>\n",
       "      <td>-0.043379</td>\n",
       "      <td>-0.042296</td>\n",
       "      <td>-0.052553</td>\n",
       "      <td>-0.043515</td>\n",
       "      <td>-0.025409</td>\n",
       "      <td>360.80</td>\n",
       "      <td>330.09</td>\n",
       "      <td>-0.085116</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-20 12:00:00</th>\n",
       "      <td>-0.076908</td>\n",
       "      <td>-0.048142</td>\n",
       "      <td>-0.061239</td>\n",
       "      <td>-0.063892</td>\n",
       "      <td>-0.036947</td>\n",
       "      <td>362.70</td>\n",
       "      <td>320.99</td>\n",
       "      <td>-0.114999</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-20 18:00:00</th>\n",
       "      <td>-0.075177</td>\n",
       "      <td>-0.105758</td>\n",
       "      <td>-0.070346</td>\n",
       "      <td>-0.101881</td>\n",
       "      <td>-0.036658</td>\n",
       "      <td>346.00</td>\n",
       "      <td>304.29</td>\n",
       "      <td>-0.120549</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     6hrs_4steps_40features_1layers_300epochs_1timesteps_0.4dropout_52units  \\\n",
       "date                                                                                          \n",
       "2017-06-19 18:00:00                                           0.013479                        \n",
       "2017-06-20 00:00:00                                          -0.015428                        \n",
       "2017-06-20 06:00:00                                          -0.043379                        \n",
       "2017-06-20 12:00:00                                          -0.076908                        \n",
       "2017-06-20 18:00:00                                          -0.075177                        \n",
       "\n",
       "                     6hrs_4steps_40features_2layers_300epochs_1timesteps_0.1dropout_128units  \\\n",
       "date                                                                                           \n",
       "2017-06-19 18:00:00                                           0.034896                         \n",
       "2017-06-20 00:00:00                                          -0.001259                         \n",
       "2017-06-20 06:00:00                                          -0.042296                         \n",
       "2017-06-20 12:00:00                                          -0.048142                         \n",
       "2017-06-20 18:00:00                                          -0.105758                         \n",
       "\n",
       "                     6hrs_4steps_40features_2layers_300epochs_1timesteps_0.1dropout_96units  \\\n",
       "date                                                                                          \n",
       "2017-06-19 18:00:00                                           0.032555                        \n",
       "2017-06-20 00:00:00                                           0.001297                        \n",
       "2017-06-20 06:00:00                                          -0.052553                        \n",
       "2017-06-20 12:00:00                                          -0.061239                        \n",
       "2017-06-20 18:00:00                                          -0.070346                        \n",
       "\n",
       "                     6hrs_4steps_40features_2layers_300epochs_1timesteps_0.2dropout_64units  \\\n",
       "date                                                                                          \n",
       "2017-06-19 18:00:00                                           0.018816                        \n",
       "2017-06-20 00:00:00                                           0.006405                        \n",
       "2017-06-20 06:00:00                                          -0.043515                        \n",
       "2017-06-20 12:00:00                                          -0.063892                        \n",
       "2017-06-20 18:00:00                                          -0.101881                        \n",
       "\n",
       "                     6hrs_4steps_80features_1layers_300epochs_1timesteps_0.2dropout_52units  \\\n",
       "date                                                                                          \n",
       "2017-06-19 18:00:00                                           0.009536                        \n",
       "2017-06-20 00:00:00                                          -0.013744                        \n",
       "2017-06-20 06:00:00                                          -0.025409                        \n",
       "2017-06-20 12:00:00                                          -0.036947                        \n",
       "2017-06-20 18:00:00                                          -0.036658                        \n",
       "\n",
       "                     usdteth_median_price  usdteth_median_price_t+4    target  \\\n",
       "date                                                                            \n",
       "2017-06-19 18:00:00                354.96                    346.00 -0.025242   \n",
       "2017-06-20 00:00:00                359.91                    341.82 -0.050263   \n",
       "2017-06-20 06:00:00                360.80                    330.09 -0.085116   \n",
       "2017-06-20 12:00:00                362.70                    320.99 -0.114999   \n",
       "2017-06-20 18:00:00                346.00                    304.29 -0.120549   \n",
       "\n",
       "                     direction  \\\n",
       "date                             \n",
       "2017-06-19 18:00:00       -1.0   \n",
       "2017-06-20 00:00:00       -1.0   \n",
       "2017-06-20 06:00:00       -1.0   \n",
       "2017-06-20 12:00:00       -1.0   \n",
       "2017-06-20 18:00:00       -1.0   \n",
       "\n",
       "                     xgb_6hrs_4steps_20features_1000estimators_20timesteps_6maxdepth_0.01learningrate  \\\n",
       "date                                                                                                    \n",
       "2017-06-19 18:00:00                                                1.0                                  \n",
       "2017-06-20 00:00:00                                               -1.0                                  \n",
       "2017-06-20 06:00:00                                               -1.0                                  \n",
       "2017-06-20 12:00:00                                               -1.0                                  \n",
       "2017-06-20 18:00:00                                               -1.0                                  \n",
       "\n",
       "                     xgb_6hrs_4steps_20features_1000estimators_20timesteps_8maxdepth_0.01learningrate  \\\n",
       "date                                                                                                    \n",
       "2017-06-19 18:00:00                                                1.0                                  \n",
       "2017-06-20 00:00:00                                               -1.0                                  \n",
       "2017-06-20 06:00:00                                               -1.0                                  \n",
       "2017-06-20 12:00:00                                               -1.0                                  \n",
       "2017-06-20 18:00:00                                               -1.0                                  \n",
       "\n",
       "                     xgb_6hrs_4steps_20features_1500estimators_20timesteps_6maxdepth_0.01learningrate  \\\n",
       "date                                                                                                    \n",
       "2017-06-19 18:00:00                                                1.0                                  \n",
       "2017-06-20 00:00:00                                               -1.0                                  \n",
       "2017-06-20 06:00:00                                               -1.0                                  \n",
       "2017-06-20 12:00:00                                               -1.0                                  \n",
       "2017-06-20 18:00:00                                               -1.0                                  \n",
       "\n",
       "                     xgb_6hrs_4steps_20features_250estimators_24timesteps_8maxdepth_0.1learningrate  \\\n",
       "date                                                                                                  \n",
       "2017-06-19 18:00:00                                                1.0                                \n",
       "2017-06-20 00:00:00                                               -1.0                                \n",
       "2017-06-20 06:00:00                                               -1.0                                \n",
       "2017-06-20 12:00:00                                               -1.0                                \n",
       "2017-06-20 18:00:00                                               -1.0                                \n",
       "\n",
       "                     xgb_6hrs_4steps_20features_500estimators_20timesteps_8maxdepth_0.01learningrate  \n",
       "date                                                                                                  \n",
       "2017-06-19 18:00:00                                                1.0                                \n",
       "2017-06-20 00:00:00                                               -1.0                                \n",
       "2017-06-20 06:00:00                                               -1.0                                \n",
       "2017-06-20 12:00:00                                               -1.0                                \n",
       "2017-06-20 18:00:00                                               -1.0                                "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the binary targets to line up date\n",
    "binary_y_true_6hr = pd.read_csv('binary_6hr_target_through-2017-08-28.csv')\n",
    "\n",
    "# set empty dict for saving and easy df conversion\n",
    "xgb_6hr_preds = {}\n",
    "\n",
    "# parse out the predicted values for each model\n",
    "for k in xgb_6hr_model_raw:\n",
    "    xgb_6hr_preds[k] = [pred[1] for pred in xgb_6hr_model_raw[k]]\n",
    "\n",
    "# save as df\n",
    "xgb_6hr_df = pd.DataFrame(xgb_6hr_preds)\n",
    "\n",
    "# set a dt version of the index for easy filtering\n",
    "binary_y_true_6hr.index = pd.to_datetime(binary_y_true_6hr['date'])\n",
    "\n",
    "# filter down to where the predictions start (2017-06-11 18:00:00)\n",
    "binary_y_true_6hr = binary_y_true_6hr[binary_y_true_6hr.index >= datetime(2017, 6, 11, 18)]\n",
    "\n",
    "# add to the df with the date index\n",
    "for col in xgb_6hr_df.columns.tolist():\n",
    "    binary_y_true_6hr[col] = xgb_6hr_df[col].values\n",
    "\n",
    "# and drop the non-index date col and save\n",
    "xgb_6hr_df = binary_y_true_6hr.drop('date', 1)\n",
    "\n",
    "# AND MERGE TOGETHER THE LSTM (6HR) WITH THE XGB (6HR) :D\n",
    "df = df.join(xgb_6hr_df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LSTM Classification (24hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_24hr_preds = {}\n",
    "\n",
    "# save the predictions of each model into a dict for easy df\n",
    "for m in lstm_24hr_model_raw.keys():\n",
    "    m_pred = [pred[0][1] for pred in lstm_24hr_model_raw[m]]\n",
    "    m_true = [pred[0][0] for pred in lstm_24hr_model_raw[m]]\n",
    "    lstm_24hr_preds[m + '_PREDICTION'] = m_pred\n",
    "    lstm_24hr_preds[m + '_ACTUAL'] = m_true\n",
    "\n",
    "# convert to df\n",
    "lstm_24hr_pred_df = pd.DataFrame(lstm_24hr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export the results to reorg manually\n",
    "#lstm_24hr_pred_df.to_csv('lstm_24hr_temp_export_to_find_starting_dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the manually organized data and drop the nans\n",
    "merged_24hr = pd.read_csv('lstm_24hr_clean.csv')\n",
    "merged_24hr = merged_24hr.dropna()\n",
    "\n",
    "pred_from_24hr = duplicate_24hr_predictions_for_6hr(merged_24hr)\n",
    "\n",
    "# read in the manually organized data and drop the nans\n",
    "merged_24hr = pd.read_csv('lstm_24hr_clean_shift_to_predict.csv')\n",
    "merged_24hr = merged_24hr.dropna()\n",
    "\n",
    "pred_to_24hr = duplicate_24hr_predictions_for_6hr(merged_24hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_24hr = pred_to_24hr.join(pred_from_24hr, lsuffix = '_TO', rsuffix = '_FROM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Merge All Dat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.join(cleaned_24hr).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SHAPE: (245, 77)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6hrs_4steps_40features_1layers_300epochs_1timesteps_0.4dropout_52units</th>\n",
       "      <th>6hrs_4steps_40features_2layers_300epochs_1timesteps_0.1dropout_128units</th>\n",
       "      <th>6hrs_4steps_40features_2layers_300epochs_1timesteps_0.1dropout_96units</th>\n",
       "      <th>6hrs_4steps_40features_2layers_300epochs_1timesteps_0.2dropout_64units</th>\n",
       "      <th>6hrs_4steps_80features_1layers_300epochs_1timesteps_0.2dropout_52units</th>\n",
       "      <th>usdteth_median_price</th>\n",
       "      <th>usdteth_median_price_t+4</th>\n",
       "      <th>target</th>\n",
       "      <th>direction</th>\n",
       "      <th>xgb_6hrs_4steps_20features_1000estimators_20timesteps_6maxdepth_0.01learningrate</th>\n",
       "      <th>...</th>\n",
       "      <th>24hrs_8steps_50features_1layers_300epochs_1timesteps_0.3dropout_64units_FROM</th>\n",
       "      <th>24hrs_8steps_50features_1layers_300epochs_1timesteps_0.3dropout_96units_FROM</th>\n",
       "      <th>9step_actual</th>\n",
       "      <th>24hrs_9steps_50features_1layers_200epochs_1timesteps_0.1dropout_96units_FROM</th>\n",
       "      <th>24hrs_9steps_50features_1layers_300epochs_1timesteps_0.3dropout_64units_FROM</th>\n",
       "      <th>24hrs_9steps_50features_1layers_300epochs_1timesteps_0.3dropout_96units_FROM</th>\n",
       "      <th>10step_actual</th>\n",
       "      <th>24hrs_10steps_50features_1layers_200epochs_1timesteps_0.1dropout_96units_FROM</th>\n",
       "      <th>24hrs_10steps_50features_1layers_300epochs_1timesteps_0.3dropout_64units_FROM</th>\n",
       "      <th>24hrs_10steps_50features_1layers_300epochs_1timesteps_0.3dropout_96units_FROM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-08-18 18:00:00</th>\n",
       "      <td>-0.008593</td>\n",
       "      <td>0.010314</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>-0.053413</td>\n",
       "      <td>291.090000</td>\n",
       "      <td>293.09</td>\n",
       "      <td>0.006871</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-19 00:00:00</th>\n",
       "      <td>-0.005088</td>\n",
       "      <td>0.028011</td>\n",
       "      <td>0.011873</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>-0.027544</td>\n",
       "      <td>295.320000</td>\n",
       "      <td>290.35</td>\n",
       "      <td>-0.016829</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-19 06:00:00</th>\n",
       "      <td>-0.013192</td>\n",
       "      <td>-0.020298</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>-0.036450</td>\n",
       "      <td>-0.023411</td>\n",
       "      <td>287.249790</td>\n",
       "      <td>294.26</td>\n",
       "      <td>0.024405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-19 12:00:00</th>\n",
       "      <td>0.018723</td>\n",
       "      <td>0.045532</td>\n",
       "      <td>0.040635</td>\n",
       "      <td>0.016210</td>\n",
       "      <td>0.022772</td>\n",
       "      <td>287.494146</td>\n",
       "      <td>294.00</td>\n",
       "      <td>0.022630</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-19 18:00:00</th>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.022419</td>\n",
       "      <td>0.011674</td>\n",
       "      <td>0.048761</td>\n",
       "      <td>-0.029013</td>\n",
       "      <td>293.090000</td>\n",
       "      <td>296.18</td>\n",
       "      <td>0.010543</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     6hrs_4steps_40features_1layers_300epochs_1timesteps_0.4dropout_52units  \\\n",
       "date                                                                                          \n",
       "2017-08-18 18:00:00                                          -0.008593                        \n",
       "2017-08-19 00:00:00                                          -0.005088                        \n",
       "2017-08-19 06:00:00                                          -0.013192                        \n",
       "2017-08-19 12:00:00                                           0.018723                        \n",
       "2017-08-19 18:00:00                                           0.001108                        \n",
       "\n",
       "                     6hrs_4steps_40features_2layers_300epochs_1timesteps_0.1dropout_128units  \\\n",
       "date                                                                                           \n",
       "2017-08-18 18:00:00                                           0.010314                         \n",
       "2017-08-19 00:00:00                                           0.028011                         \n",
       "2017-08-19 06:00:00                                          -0.020298                         \n",
       "2017-08-19 12:00:00                                           0.045532                         \n",
       "2017-08-19 18:00:00                                           0.022419                         \n",
       "\n",
       "                     6hrs_4steps_40features_2layers_300epochs_1timesteps_0.1dropout_96units  \\\n",
       "date                                                                                          \n",
       "2017-08-18 18:00:00                                           0.004281                        \n",
       "2017-08-19 00:00:00                                           0.011873                        \n",
       "2017-08-19 06:00:00                                           0.001502                        \n",
       "2017-08-19 12:00:00                                           0.040635                        \n",
       "2017-08-19 18:00:00                                           0.011674                        \n",
       "\n",
       "                     6hrs_4steps_40features_2layers_300epochs_1timesteps_0.2dropout_64units  \\\n",
       "date                                                                                          \n",
       "2017-08-18 18:00:00                                           0.001965                        \n",
       "2017-08-19 00:00:00                                           0.000457                        \n",
       "2017-08-19 06:00:00                                          -0.036450                        \n",
       "2017-08-19 12:00:00                                           0.016210                        \n",
       "2017-08-19 18:00:00                                           0.048761                        \n",
       "\n",
       "                     6hrs_4steps_80features_1layers_300epochs_1timesteps_0.2dropout_52units  \\\n",
       "date                                                                                          \n",
       "2017-08-18 18:00:00                                          -0.053413                        \n",
       "2017-08-19 00:00:00                                          -0.027544                        \n",
       "2017-08-19 06:00:00                                          -0.023411                        \n",
       "2017-08-19 12:00:00                                           0.022772                        \n",
       "2017-08-19 18:00:00                                          -0.029013                        \n",
       "\n",
       "                     usdteth_median_price  usdteth_median_price_t+4    target  \\\n",
       "date                                                                            \n",
       "2017-08-18 18:00:00            291.090000                    293.09  0.006871   \n",
       "2017-08-19 00:00:00            295.320000                    290.35 -0.016829   \n",
       "2017-08-19 06:00:00            287.249790                    294.26  0.024405   \n",
       "2017-08-19 12:00:00            287.494146                    294.00  0.022630   \n",
       "2017-08-19 18:00:00            293.090000                    296.18  0.010543   \n",
       "\n",
       "                     direction  \\\n",
       "date                             \n",
       "2017-08-18 18:00:00        1.0   \n",
       "2017-08-19 00:00:00       -1.0   \n",
       "2017-08-19 06:00:00        1.0   \n",
       "2017-08-19 12:00:00        1.0   \n",
       "2017-08-19 18:00:00        1.0   \n",
       "\n",
       "                     xgb_6hrs_4steps_20features_1000estimators_20timesteps_6maxdepth_0.01learningrate  \\\n",
       "date                                                                                                    \n",
       "2017-08-18 18:00:00                                               -1.0                                  \n",
       "2017-08-19 00:00:00                                               -1.0                                  \n",
       "2017-08-19 06:00:00                                               -1.0                                  \n",
       "2017-08-19 12:00:00                                                1.0                                  \n",
       "2017-08-19 18:00:00                                                1.0                                  \n",
       "\n",
       "                                                         ...                                        \\\n",
       "date                                                     ...                                         \n",
       "2017-08-18 18:00:00                                      ...                                         \n",
       "2017-08-19 00:00:00                                      ...                                         \n",
       "2017-08-19 06:00:00                                      ...                                         \n",
       "2017-08-19 12:00:00                                      ...                                         \n",
       "2017-08-19 18:00:00                                      ...                                         \n",
       "\n",
       "                     24hrs_8steps_50features_1layers_300epochs_1timesteps_0.3dropout_64units_FROM  \\\n",
       "date                                                                                                \n",
       "2017-08-18 18:00:00                                                1.0                              \n",
       "2017-08-19 00:00:00                                                1.0                              \n",
       "2017-08-19 06:00:00                                                1.0                              \n",
       "2017-08-19 12:00:00                                                1.0                              \n",
       "2017-08-19 18:00:00                                                1.0                              \n",
       "\n",
       "                     24hrs_8steps_50features_1layers_300epochs_1timesteps_0.3dropout_96units_FROM  \\\n",
       "date                                                                                                \n",
       "2017-08-18 18:00:00                                                1.0                              \n",
       "2017-08-19 00:00:00                                                1.0                              \n",
       "2017-08-19 06:00:00                                                1.0                              \n",
       "2017-08-19 12:00:00                                                1.0                              \n",
       "2017-08-19 18:00:00                                                1.0                              \n",
       "\n",
       "                     9step_actual  \\\n",
       "date                                \n",
       "2017-08-18 18:00:00           1.0   \n",
       "2017-08-19 00:00:00           1.0   \n",
       "2017-08-19 06:00:00           1.0   \n",
       "2017-08-19 12:00:00           1.0   \n",
       "2017-08-19 18:00:00           1.0   \n",
       "\n",
       "                     24hrs_9steps_50features_1layers_200epochs_1timesteps_0.1dropout_96units_FROM  \\\n",
       "date                                                                                                \n",
       "2017-08-18 18:00:00                                                1.0                              \n",
       "2017-08-19 00:00:00                                                1.0                              \n",
       "2017-08-19 06:00:00                                                1.0                              \n",
       "2017-08-19 12:00:00                                                1.0                              \n",
       "2017-08-19 18:00:00                                                1.0                              \n",
       "\n",
       "                     24hrs_9steps_50features_1layers_300epochs_1timesteps_0.3dropout_64units_FROM  \\\n",
       "date                                                                                                \n",
       "2017-08-18 18:00:00                                                1.0                              \n",
       "2017-08-19 00:00:00                                                1.0                              \n",
       "2017-08-19 06:00:00                                                1.0                              \n",
       "2017-08-19 12:00:00                                                1.0                              \n",
       "2017-08-19 18:00:00                                                1.0                              \n",
       "\n",
       "                     24hrs_9steps_50features_1layers_300epochs_1timesteps_0.3dropout_96units_FROM  \\\n",
       "date                                                                                                \n",
       "2017-08-18 18:00:00                                                1.0                              \n",
       "2017-08-19 00:00:00                                                1.0                              \n",
       "2017-08-19 06:00:00                                                1.0                              \n",
       "2017-08-19 12:00:00                                                1.0                              \n",
       "2017-08-19 18:00:00                                                1.0                              \n",
       "\n",
       "                     10step_actual  \\\n",
       "date                                 \n",
       "2017-08-18 18:00:00            1.0   \n",
       "2017-08-19 00:00:00            1.0   \n",
       "2017-08-19 06:00:00            1.0   \n",
       "2017-08-19 12:00:00            1.0   \n",
       "2017-08-19 18:00:00            1.0   \n",
       "\n",
       "                     24hrs_10steps_50features_1layers_200epochs_1timesteps_0.1dropout_96units_FROM  \\\n",
       "date                                                                                                 \n",
       "2017-08-18 18:00:00                                           1.000000                               \n",
       "2017-08-19 00:00:00                                           0.999986                               \n",
       "2017-08-19 06:00:00                                           0.999986                               \n",
       "2017-08-19 12:00:00                                           0.999986                               \n",
       "2017-08-19 18:00:00                                           0.999986                               \n",
       "\n",
       "                     24hrs_10steps_50features_1layers_300epochs_1timesteps_0.3dropout_64units_FROM  \\\n",
       "date                                                                                                 \n",
       "2017-08-18 18:00:00                                           0.999998                               \n",
       "2017-08-19 00:00:00                                           1.000000                               \n",
       "2017-08-19 06:00:00                                           1.000000                               \n",
       "2017-08-19 12:00:00                                           1.000000                               \n",
       "2017-08-19 18:00:00                                           1.000000                               \n",
       "\n",
       "                     24hrs_10steps_50features_1layers_300epochs_1timesteps_0.3dropout_96units_FROM  \n",
       "date                                                                                                \n",
       "2017-08-18 18:00:00                                                1.0                              \n",
       "2017-08-19 00:00:00                                                1.0                              \n",
       "2017-08-19 06:00:00                                                1.0                              \n",
       "2017-08-19 12:00:00                                                1.0                              \n",
       "2017-08-19 18:00:00                                                1.0                              \n",
       "\n",
       "[5 rows x 77 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nSHAPE:', df.shape)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Progress:\n",
    "\n",
    " - Finished gathering 6hr LSTM\n",
    " - Finished gathering 6hr XGB\n",
    " - Merged the 6hr LSTM & XGB\n",
    " - Also running the predictions for the 24hr LSTM. That should be done by the afternoon (if nothing dies)\n",
    " \n",
    " - After the above is finished:\n",
    "     - Reorganize the 24hr LSTM predictions and duplicate them so they can align with 6hr\n",
    "         - Do it both ways, for current and for future (staggering)\n",
    "     - Align 6hr XGB predictions\n",
    " - Join all predictions\n",
    " - Clean up the dataset\n",
    " - Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict w/ [INSERT_ALGO_TYPE_HERE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wfw = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_types = [\n",
    "    lstm_6hr_regr_cols,\n",
    "    xgb_6hr_clf_cols,\n",
    "    lstm_24hr_clf_to_cols,\n",
    "    lstm_24hr_clf_from_cols,\n",
    "]\n",
    "\n",
    "cols_to_include = []\n",
    "\n",
    "for col_type in col_types:\n",
    "    cols_to_include += col_type\n",
    "cols_to_include += ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:5: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# filter down to cols w/ preds only\n",
    "fdf = df[cols_to_include]\n",
    "\n",
    "# set X and y\n",
    "X = fdf.ix[:,:-1].as_matrix()\n",
    "y = fdf.ix[:,-1].as_matrix()\n",
    "\n",
    "\n",
    "# init standard scaler\n",
    "X_standard_scaler = StandardScaler()\n",
    "y_standard_scaler = StandardScaler()\n",
    "\n",
    "# standardize X and Y\n",
    "standardized_X = X_standard_scaler.fit_transform(X)\n",
    "standardized_y = y_standard_scaler.fit_transform(y)\n",
    "\n",
    "\n",
    "# init minmax scaler\n",
    "X_minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# scale X and Y\n",
    "scaled_X = X_minmax_scaler.fit_transform(X)\n",
    "scaled_y = y_minmax_scaler.fit_transform(y)\n",
    "\n",
    "# test out polynomial features\n",
    "poly = PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)\n",
    "poly_X = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: (245, 64)\n",
      "shape of poly X: (245, 2080)\n"
     ]
    }
   ],
   "source": [
    "print('shape of X:', X.shape)\n",
    "print('shape of poly X:', poly_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0389022390611\n",
      "RMSE: 0.0506513394149\n",
      "Sign Accuracy: 0.808163265306\n"
     ]
    }
   ],
   "source": [
    "# return perormance metrics\n",
    "regression_kpis(list(zip(fdf.target, df[baseline_cols].mean(1).tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (245, 64)\n",
      "y Shape: (245,)\n"
     ]
    }
   ],
   "source": [
    "print('X Shape:', X.shape)\n",
    "print('y Shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW INPUT\n",
      "complete!\n",
      "MAE: 11906808065.2\n",
      "RMSE: 49107432612.5\n",
      "Sign Accuracy: 0.696\n",
      "\n",
      "STANDARDIZED INPUT\n",
      "complete!\n",
      "MAE: 9274559092.04\n",
      "RMSE: 35291046375.9\n",
      "Sign Accuracy: 0.68\n",
      "\n",
      "SCALED INPUT\n",
      "complete!\n",
      "MAE: 5494337356.19\n",
      "RMSE: 19025176950.4\n",
      "Sign Accuracy: 0.648\n",
      "POLY INPUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete!\n",
      "MAE: 0.125080246371\n",
      "RMSE: 0.18032036061\n",
      "Sign Accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "print('RAW INPUT')\n",
    "# get prediction list\n",
    "pl = sklearn_wfv_regression(X, y, LinearRegression(), wfw, verbose = False)\n",
    "\n",
    "# return perormance metrics\n",
    "regression_kpis(pl)\n",
    "\n",
    "print()\n",
    "\n",
    "print('STANDARDIZED INPUT')\n",
    "# get prediction list\n",
    "pl = sklearn_wfv_regression(standardized_X, standardized_y, LinearRegression(), wfw, verbose = False)\n",
    "\n",
    "# return perormance metrics\n",
    "regression_kpis(pl, y_standard_scaler)\n",
    "\n",
    "print()\n",
    "\n",
    "print('SCALED INPUT')\n",
    "# get prediction list\n",
    "pl = sklearn_wfv_regression(scaled_X, scaled_y, LinearRegression(), wfw, verbose = False)\n",
    "\n",
    "# return perormance metrics\n",
    "regression_kpis(pl, y_minmax_scaler)\n",
    "\n",
    "print('POLY INPUT')\n",
    "# get prediction list\n",
    "pl = sklearn_wfv_regression(poly_X, y, LinearRegression(), wfw, verbose = False)\n",
    "\n",
    "# return perormance metrics\n",
    "regression_kpis(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn NN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW INPUT\n",
      "complete!\n",
      "MAE: 0.114354320933\n",
      "RMSE: 0.154675162788\n",
      "Sign Accuracy: 0.568\n",
      "\n",
      "STANDARDIZED INPUT\n",
      "complete!\n",
      "MAE: 0.038550235194\n",
      "RMSE: 0.05067314203\n",
      "Sign Accuracy: 0.728\n",
      "\n",
      "SCALED INPUT\n",
      "complete!\n",
      "MAE: 0.0526837322194\n",
      "RMSE: 0.065450698371\n",
      "Sign Accuracy: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:374: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#print('RAW INPUT')\n",
    "# get prediction list\n",
    "#pl = sklearn_wfv_regression(X, y, MLPRegressor(batch_size = 1), wfw, verbose = False)\n",
    "# return perormance metrics\n",
    "#regression_kpis(pl)\n",
    "#print()\n",
    "\n",
    "print('STANDARDIZED INPUT')\n",
    "# get prediction list\n",
    "pl = sklearn_wfv_regression(standardized_X, standardized_y, MLPRegressor(batch_size = 1), wfw, verbose = False)\n",
    "\n",
    "# return perormance metrics\n",
    "regression_kpis(pl, y_standard_scaler)\n",
    "\n",
    "print()\n",
    "\n",
    "print('SCALED INPUT')\n",
    "# get prediction list\n",
    "pl = sklearn_wfv_regression(scaled_X, scaled_y, MLPRegressor(batch_size = 1), wfw, verbose = False)\n",
    "\n",
    "# return perormance metrics\n",
    "regression_kpis(pl, y_minmax_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete!\n",
      "MAE: 0.033141612177\n",
      "RMSE: 0.0427581290105\n",
      "Sign Accuracy: 0.736\n"
     ]
    }
   ],
   "source": [
    "# get prediction list\n",
    "pl = sklearn_wfv_regression(X, y, RandomForestRegressor(), wfw, verbose = False)\n",
    "\n",
    "# return perormance metrics\n",
    "regression_kpis(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini RF Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete!\n",
      "\n",
      "40Estimators_10MaxDepth\n",
      "MAE: 0.0297345161387\n",
      "RMSE: 0.0397834987169\n",
      "Sign Accuracy: 0.728\n",
      "complete!\n",
      "\n",
      "40Estimators_20MaxDepth\n",
      "MAE: 0.0302394682319\n",
      "RMSE: 0.0404184489603\n",
      "Sign Accuracy: 0.752\n",
      "complete!\n",
      "\n",
      "40Estimators_30MaxDepth\n",
      "MAE: 0.0287942270257\n",
      "RMSE: 0.0387464577814\n",
      "Sign Accuracy: 0.744\n",
      "complete!\n",
      "\n",
      "40Estimators_40MaxDepth\n",
      "MAE: 0.0300642404448\n",
      "RMSE: 0.0406639073046\n",
      "Sign Accuracy: 0.752\n",
      "complete!\n",
      "\n",
      "40Estimators_50MaxDepth\n",
      "MAE: 0.0298429688774\n",
      "RMSE: 0.040317985352\n",
      "Sign Accuracy: 0.744\n",
      "complete!\n",
      "\n",
      "50Estimators_10MaxDepth\n",
      "MAE: 0.0296077625623\n",
      "RMSE: 0.0395037987755\n",
      "Sign Accuracy: 0.736\n",
      "complete!\n",
      "\n",
      "50Estimators_20MaxDepth\n",
      "MAE: 0.0294467685121\n",
      "RMSE: 0.0405240051644\n",
      "Sign Accuracy: 0.736\n",
      "complete!\n",
      "\n",
      "50Estimators_30MaxDepth\n",
      "MAE: 0.028511423171\n",
      "RMSE: 0.0386141532792\n",
      "Sign Accuracy: 0.76\n",
      "complete!\n",
      "\n",
      "50Estimators_40MaxDepth\n",
      "MAE: 0.0294667930065\n",
      "RMSE: 0.0398043902598\n",
      "Sign Accuracy: 0.752\n",
      "complete!\n",
      "\n",
      "50Estimators_50MaxDepth\n",
      "MAE: 0.0298261444218\n",
      "RMSE: 0.0394517401842\n",
      "Sign Accuracy: 0.784\n",
      "complete!\n",
      "\n",
      "60Estimators_10MaxDepth\n",
      "MAE: 0.0296586652138\n",
      "RMSE: 0.0392074525396\n",
      "Sign Accuracy: 0.736\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d0fc8f105cd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# get prediction list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn_wfv_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwfw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-07f9867ef52b>\u001b[0m in \u001b[0;36msklearn_wfv_regression\u001b[0;34m(X, y, _model, walk_forward_window, verbose)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# fit model no training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# make predictions for test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 326\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1030\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    348\u001b[0m                                            self.min_impurity_split)\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_estimators = [40, 50, 60, 70, 80]\n",
    "max_depth = [10, 20, 30, 40, 50]\n",
    "\n",
    "for est in n_estimators:\n",
    "    for dep in max_depth:\n",
    "        \n",
    "        rf_id = '%sEstimators_%sMaxDepth' % (est, dep)\n",
    "        \n",
    "        # get prediction list\n",
    "        pl = sklearn_wfv_regression(X, y, RandomForestRegressor(n_estimators = est, max_depth = dep), wfw, verbose = False)\n",
    "        \n",
    "        print()\n",
    "        print(rf_id)\n",
    "        \n",
    "        # return perormance metrics\n",
    "        regression_kpis(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini RF Grid Search (w/ Polynomial Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [1000, 2500]\n",
    "max_depth = [50]\n",
    "\n",
    "for est in n_estimators:\n",
    "    for dep in max_depth:\n",
    "        \n",
    "        rf_id = '%sEstimators_%sMaxDepth' % (est, dep)\n",
    "        \n",
    "        # get prediction list\n",
    "        pl = sklearn_wfv_regression(poly_X, y, RandomForestRegressor(n_estimators = est, max_depth = dep), wfw, verbose = 2)\n",
    "        \n",
    "        print()\n",
    "        print(rf_id)\n",
    "        \n",
    "        # return perormance metrics\n",
    "        regression_kpis(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Best Ensembles\n",
    "\n",
    "**Random Forest**\n",
    " - Features: all models except \"FROM\"\n",
    "     - 32Estimators_10MaxDepth: 2.9862% MAE\n",
    "     - 32Estimators_20MaxDepth: 2.9865% MAE\n",
    " - Features: all models\n",
    "     - 45Estimators_25MaxDepth: 2.8292% MAE\n",
    "     - 40Estimators_40MaxDepth: 2.8421% MAE\n",
    "     - 50Estimators_35MaxDepth: 2.8430% MAE\n",
    "     - 40Estimators_10MaxDepth: 2.8471% MAE\n",
    "     - 50Estimators_30MaxDepth: 2.8511% MAE\n",
    "     - 35Estimators_10MaxDepth: 2.8516% MAE\n",
    "     - 35Estimators_15MaxDepth: 2.8524% MAE\n",
    "     \n",
    "\n",
    "**Linear Regression**\n",
    " - 6hr LSTM regression\n",
    "     - 3.0821% MAE\n",
    " - 6hr LSTM regression, 6hr XGB classification\n",
    "     - 3.2053% MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Miscelaneous Analyses\n",
    "### What Happens When I Use All LSTM 6hr Models in a Random Forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "actual = {}\n",
    "\n",
    "for m in all_lstm_6hr_regr_models:\n",
    "    pred = [y[0][1] for y in all_lstm_6hr_regr_models[m]]\n",
    "    true = [y[0][0] for y in all_lstm_6hr_regr_models[m]]\n",
    "    predictions[m] = pred\n",
    "    actual[m] = true\n",
    "\n",
    "all_lstm_6hr_df = pd.DataFrame(predictions)\n",
    "all_lstm_6hr_df['target'] = pd.DataFrame(actual).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# set X and y\n",
    "X = all_lstm_6hr_df.ix[:,:-1].as_matrix()\n",
    "y = all_lstm_6hr_df.ix[:,-1].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete!\n",
      "MAE: 0.0343085512257\n",
      "RMSE: 0.0439898016517\n",
      "Sign Accuracy: 0.696\n"
     ]
    }
   ],
   "source": [
    "##### get prediction list\n",
    "pl = sklearn_wfv_regression(X, y, RandomForestRegressor(), wfw, verbose = False)\n",
    "\n",
    "# return perormance metrics\n",
    "regression_kpis(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# n_estimators = [25, 50, 75, 100]\n",
    "# max_depth = [25, 50, 75]\n",
    "\n",
    "# for est in n_estimators:\n",
    "#     for dep in max_depth:\n",
    "        \n",
    "#         rf_id = '%sEstimators_%sMaxDepth' % (est, dep)\n",
    "        \n",
    "#         # get prediction list\n",
    "#         pl = sklearn_wfv_regression(X, y, RandomForestRegressor(n_estimators = est, max_depth = dep), wfw, verbose = False)\n",
    "        \n",
    "#         print()\n",
    "#         print(rf_id)\n",
    "        \n",
    "#         # return perormance metrics\n",
    "#         regression_kpis(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Random Forest to Select the Top Models for Use in the Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {'n_estimators': 75, 'max_depth': 25}\n",
    "\n",
    "ranked_models = get_ranked_features(X, y,\n",
    "                                    hyperparams,\n",
    "                                    all_lstm_6hr_df.iloc[:,:-1].columns.tolist(),\n",
    "                                    nb_epochs = 250\n",
    "                                   )\n",
    "\n",
    "pd.DataFrame(ranked_models)[:10].iloc[:,0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Linear Regression to Select the Top Models for Use in the Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# init standard scaler\n",
    "X_standard_scaler = StandardScaler()\n",
    "y_standard_scaler = StandardScaler()\n",
    "\n",
    "# standardize X and Y to analyze coefs\n",
    "standardized_X = X_standard_scaler.fit_transform(X)\n",
    "standardized_y = y_standard_scaler.fit_transform(y)\n",
    "\n",
    "# fit a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(standardized_X, standardized_y)\n",
    "\n",
    "# get the coeficients of each model\n",
    "lr_coef = [{'model': m, 'coef': c} for m, c in zip(all_lstm_6hr_df.iloc[:,:-1].columns.tolist(), list(model.coef_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lr_coef).sort_values('coef', ascending = False).model.tolist()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top unique models, as per rf and lr\n",
    "list(set(top_lstm_6hr_models_as_per_rf + top_lstm_6hr_models_as_per_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
