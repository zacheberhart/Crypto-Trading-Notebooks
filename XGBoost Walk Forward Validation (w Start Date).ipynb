{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zach-eberhart/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# STANDARD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, DMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper to get a specific number of features\n",
    "def set_features(df, target_col, n_features):\n",
    "    \n",
    "    # get list of features and slice the n we need, then add back target\n",
    "    features_list = [col for col in df.columns if col != target_col][:n_features]\n",
    "    features_list.append(target_col)\n",
    "    \n",
    "    return features_list\n",
    "\n",
    "# reshape data so that LSTM doesn't get mad\n",
    "def reshape_data(array, time_steps):\n",
    "    \n",
    "    # set length / number of samples\n",
    "    L = array.shape[0] - time_steps + 1\n",
    "    \n",
    "    # use strides for something\n",
    "    strided = np.lib.stride_tricks.as_strided\n",
    "    m, n = array.strides\n",
    "    \n",
    "    # set width / number of features\n",
    "    N = array.shape[1]\n",
    "    \n",
    "    # and finally, reshape data according to specifications\n",
    "    reshaped = strided(array, shape = (L, time_steps, N), strides = (N * n, m, n))\n",
    "    \n",
    "    return reshaped.copy()\n",
    "\n",
    "# get X, Y, and column headers / names for random forest\n",
    "def get_lstm_matrix(df, target_col, periods_ahead, drop_target = False, rate_of_change = False):\n",
    "    \n",
    "    # read in all cols with closing prices\n",
    "    df = df[df[target_col].notnull()]\n",
    "    \n",
    "    # clean data for BTC_ETH starts on 2015-08-09\n",
    "    df = df[df.index >= datetime(2015, 8, 9)]\n",
    "    \n",
    "    # select target column and create it in dataset\n",
    "    target_col_name = target_col + '_target'\n",
    "    df[target_col_name] = df[target_col].shift(-periods_ahead)\n",
    "    \n",
    "    # if looking at the rate of change instead of actual values\n",
    "    if rate_of_change:\n",
    "        df[target_col_name] = (df[target_col_name] - df[target_col]) / df[target_col]\n",
    "    \n",
    "    # drop the original target col\n",
    "    if drop_target:\n",
    "        df = df.drop(target_col, 1)\n",
    "    \n",
    "    # drop null values\n",
    "    df.dropna(inplace = True)\n",
    "    \n",
    "    # convert to matrix\n",
    "    dat = df.as_matrix()\n",
    "    \n",
    "    # convert to float (just in case)\n",
    "    dat = dat.astype('float32')\n",
    "    \n",
    "    # get dates for QA\n",
    "    dates = df.index.tolist()\n",
    "    \n",
    "    return df, dat, dates\n",
    "\n",
    "# get lagged version of X (straight up, no RoC)\n",
    "def get_lagged_x_straight(_array, timesteps):\n",
    "    \n",
    "    # reshpae to add timesteps\n",
    "    reshaped_array = reshape_data(_array, timesteps)\n",
    "    \n",
    "    # flatten to two dimensions\n",
    "    flattened_array = np.array([list(sub_arr.reshape(1,-1)[0]) for sub_arr in reshaped_array])\n",
    "    \n",
    "    return flattened_array\n",
    "\n",
    "# get a lagged version of the dataset for a specific n of timesteps\n",
    "def get_lagged_dataset(df, target_col, periods_ahead, n_features, timesteps, roc):\n",
    "    \n",
    "    # filter dowm to n features needed\n",
    "    df = df[set_features(df, target_col, n_features)]\n",
    "    \n",
    "    # pre-process data\n",
    "    adf, dat, dates = get_lstm_matrix(df,\n",
    "                                      target_col = target_col,\n",
    "                                      periods_ahead = periods_ahead,\n",
    "                                      drop_target = True,\n",
    "                                      rate_of_change = True\n",
    "                                     )\n",
    "    \n",
    "    # add the target col\n",
    "    adf['direction'] = adf[target_col + '_target'].apply(lambda x: np.sign(x))\n",
    "\n",
    "    # reverse order of df so it is more intuitive\n",
    "    adf = adf.sort_index(ascending = False)\n",
    "\n",
    "    # set X and y\n",
    "    X = adf.ix[:,:-2].as_matrix().copy()\n",
    "    y = adf.ix[:,-1].as_matrix().copy()\n",
    "    \n",
    "    if roc:\n",
    "        # transform X to add n lag\n",
    "        X = get_lagged_x_roc(X, timesteps).copy()\n",
    "    elif not roc:\n",
    "        # transform X to add n lag\n",
    "        X = get_lagged_x_straight(X, timesteps).copy()\n",
    "        \n",
    "    else: 'Please choose lag type!'\n",
    "    \n",
    "    # trim y to match X\n",
    "    y = y[:-timesteps+1].copy()\n",
    "    \n",
    "    return X, y, adf\n",
    "\n",
    "def get_wf_start_date(df, param_dict, _target, _wfw, _periods_ahead):\n",
    "\n",
    "    _target = _target\n",
    "    walk_forward_window = _wfw\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    ''''''''''''''\n",
    "    # set empty dict for grid search results\n",
    "    iter_results = {}\n",
    "\n",
    "    # loop through each model\n",
    "    for ens_model in range(len(xgboost_lagged_ens)):\n",
    "\n",
    "        # get current iterations model\n",
    "        current_model = xgboost_lagged_ens[ens_model]\n",
    "\n",
    "        # set hyperparams\n",
    "        _features = int(param_dict['features'])\n",
    "        _timesteps = param_dict['timesteps']\n",
    "        \n",
    "        # get the data\n",
    "        X, y, ndf = get_lagged_dataset(df,\n",
    "                                       target_col = _target,\n",
    "                                       periods_ahead = _periods_ahead,\n",
    "                                       n_features = _features,\n",
    "                                       timesteps = _timesteps,\n",
    "                                       roc = False\n",
    "                                      )\n",
    "\n",
    "        # the get_lagged_dataset() func returns the data flipped for intuitive testing\n",
    "        # but it needs to go in ascending order for walk forward validation, so flip it!\n",
    "        X = np.flipud(X).copy()\n",
    "        y = np.flipud(y).copy()\n",
    "        ndf = ndf.sort_index()\n",
    "\n",
    "        prediction_list = []\n",
    "\n",
    "        for wf in list(reversed(range(1, walk_forward_window + 1))):\n",
    "\n",
    "            # get the test y\n",
    "            test_y = y[-wf].copy()\n",
    "\n",
    "            print('Starting Date:', ndf.index[-wf])\n",
    "            print('Target:', test_y)\n",
    "            print()\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in training data\n",
    "df6 = pd.read_csv('../_trainingData/train_filtered_6hr_2017-09-27.csv')\n",
    "df24 = pd.read_csv('../_trainingData/24hr_newpair_train_filtered.csv')\n",
    "\n",
    "# clean import\n",
    "df6.index = pd.to_datetime(df6.date)\n",
    "df24.index = pd.to_datetime(df24.date)\n",
    "df6 = df6.drop('date', 1)\n",
    "df24 = df24.drop('date', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #this is the export from the grid search df, reformat this below for validation\n",
    "# top_xgb_model_records = [\n",
    "    \n",
    "#     # top six\n",
    "#     {'timesteps': '24', 'model': '6hrs_4steps_straight-lag_20features_24timesteps', 'features': '20', 'n_estimators': 1500, 'lag_type': 'straight-lag', 'max_depth': 20, 'learning_rate': 0.005},\n",
    "#     {'timesteps': '24', 'model': '6hrs_4steps_straight-lag_20features_24timesteps', 'features': '20', 'n_estimators': 1000, 'lag_type': 'straight-lag', 'max_depth': 30, 'learning_rate': 0.01},\n",
    "#     {'timesteps': '24', 'model': '6hrs_4steps_straight-lag_20features_24timesteps', 'features': '20', 'n_estimators': 1000, 'lag_type': 'straight-lag', 'max_depth': 35, 'learning_rate': 0.01},\n",
    "#     {'timesteps': '24', 'model': '6hrs_4steps_straight-lag_20features_24timesteps', 'features': '20', 'n_estimators': 1000, 'lag_type': 'straight-lag', 'max_depth': 25, 'learning_rate': 0.01},\n",
    "#     {'timesteps': '24', 'model': '6hrs_4steps_straight-lag_20features_24timesteps', 'features': '20', 'n_estimators': 2000, 'lag_type': 'straight-lag', 'max_depth': 20, 'learning_rate': 0.005},\n",
    "#     {'timesteps': '24', 'model': '6hrs_4steps_straight-lag_20features_24timesteps', 'features': '20', 'n_estimators': 2000, 'lag_type': 'straight-lag', 'max_depth': 15, 'learning_rate': 0.005},\n",
    "    \n",
    "#     # next top six\n",
    "#     {'timesteps': '24', 'model': '6hrs_4steps_straight-lag_20features_24timesteps', 'features': '20', 'n_estimators': 1500, 'lag_type': 'straight-lag', 'max_depth': 15, 'learning_rate': 0.005},\n",
    "#     {'timesteps': '24', 'model': '6hrs_4steps_straight-lag_20features_24timesteps', 'features': '20', 'n_estimators': 2000, 'lag_type': 'straight-lag', 'max_depth': 30, 'learning_rate': 0.005},\n",
    "#     {'timesteps': '24', 'model': '6hrs_4steps_straight-lag_20features_24timesteps', 'features': '20', 'n_estimators': 2000, 'lag_type': 'straight-lag', 'max_depth': 35, 'learning_rate': 0.005},\n",
    "#     {'timesteps': '24', 'model': '6hrs_4steps_straight-lag_20features_24timesteps', 'features': '20', 'n_estimators': 2000, 'lag_type': 'straight-lag', 'max_depth': 25, 'learning_rate': 0.005},\n",
    "#     {'timesteps': '24', 'model': '6hrs_4steps_straight-lag_20features_24timesteps', 'features': '20', 'n_estimators': 1500, 'lag_type': 'straight-lag', 'max_depth': 35, 'learning_rate': 0.005},\n",
    "#     {'timesteps': '24', 'model': '6hrs_4steps_straight-lag_20features_24timesteps', 'features': '20', 'n_estimators': 1500, 'lag_type': 'straight-lag', 'max_depth': 25, 'learning_rate': 0.005}\n",
    "    \n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model params in ensemble\n",
    "xgboost_lagged_ens = [\n",
    "    \n",
    "    {'features': 20, 'timesteps': 24, 'max_depth': 20, 'learning_rate': 0.005, 'n_estimators': 1500},\n",
    "    {'features': 20, 'timesteps': 24, 'max_depth': 30, 'learning_rate': 0.01, 'n_estimators': 1000},\n",
    "    {'features': 20, 'timesteps': 24, 'max_depth': 35, 'learning_rate': 0.01, 'n_estimators': 1000},\n",
    "    {'features': 20, 'timesteps': 24, 'max_depth': 25, 'learning_rate': 0.01, 'n_estimators': 1000},\n",
    "    {'features': 20, 'timesteps': 24, 'max_depth': 20, 'learning_rate': 0.005, 'n_estimators': 2000},\n",
    "    {'features': 20, 'timesteps': 24, 'max_depth': 15, 'learning_rate': 0.005, 'n_estimators': 2000},\n",
    "    {'features': 20, 'timesteps': 24, 'max_depth': 15, 'learning_rate': 0.005, 'n_estimators': 1500},\n",
    "    {'features': 20, 'timesteps': 24, 'max_depth': 30, 'learning_rate': 0.005, 'n_estimators': 2000},\n",
    "    {'features': 20, 'timesteps': 24, 'max_depth': 35, 'learning_rate': 0.005, 'n_estimators': 2000},\n",
    "    {'features': 20, 'timesteps': 24, 'max_depth': 25, 'learning_rate': 0.005, 'n_estimators': 2000},\n",
    "    {'features': 20, 'timesteps': 24, 'max_depth': 35, 'learning_rate': 0.005, 'n_estimators': 1500},\n",
    "    {'features': 20, 'timesteps': 24, 'max_depth': 25, 'learning_rate': 0.005, 'n_estimators': 1500}\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "_target = 'polo_usdteth_median_trade_price'\n",
    "periods_ahead = 4\n",
    "walk_forward_window = 140\n",
    "\n",
    "''''''''''''''\n",
    "# set empty dict for grid search results\n",
    "iter_results = {}\n",
    "\n",
    "# loop through each model\n",
    "for ens_model in range(len(xgboost_lagged_ens)):\n",
    "    \n",
    "    # get current iterations model\n",
    "    current_model = xgboost_lagged_ens[ens_model]\n",
    "    \n",
    "    # set hyperparams\n",
    "    _features = int(current_model['features']),\n",
    "    _timesteps = current_model['timesteps'],\n",
    "    _max_depth = current_model['max_depth'],\n",
    "    _learning_rate = current_model['learning_rate'],\n",
    "    _n_estimators = current_model['n_estimators']\n",
    "    \n",
    "    _features = int(_features[0])\n",
    "    _timesteps = int(_timesteps[0])\n",
    "    _max_depth = int(_max_depth[0])\n",
    "    _learning_rate = _learning_rate[0]\n",
    "    #_n_estimators = int(_n_estimators[0])\n",
    "    \n",
    "    # set the hyperparams of the xgboost model\n",
    "    xgb_params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'max_depth': _max_depth,\n",
    "        'n_estimators': _n_estimators,\n",
    "        'learning_rate': _learning_rate\n",
    "    }\n",
    "    \n",
    "    # set model ID\n",
    "    id_tuple = (_features, _n_estimators, _timesteps, _max_depth, _learning_rate)\n",
    "    iter_id = 'xgb_6hrs_4steps_%sfeatures_%sestimators_%stimesteps_%smaxdepth_%slearningrate' % id_tuple\n",
    "    \n",
    "    # progress print\n",
    "    print('starting', iter_id)\n",
    "\n",
    "    # get the data\n",
    "    X, y, tdf = get_lagged_dataset(df6,\n",
    "                                   target_col = _target,\n",
    "                                   periods_ahead = periods_ahead,\n",
    "                                   n_features = _features,\n",
    "                                   timesteps = _timesteps,\n",
    "                                   roc = False\n",
    "                                  )\n",
    "    \n",
    "    # this is returned for the start date check, just delete it for actual testing\n",
    "    del tdf\n",
    "    \n",
    "    # the get_lagged_dataset() func returns the data flipped for intuitive testing\n",
    "    # but it needs to go in ascending order for walk forward validation, so flip it!\n",
    "    X = np.flipud(X).copy()\n",
    "    y = np.flipud(y).copy()\n",
    "    \n",
    "    '''\n",
    "    TEST ARRAY (FOR... TESTING)\n",
    "    '''\n",
    "    ## get X and Y\n",
    "    #X = test_array[:,:-1].copy()\n",
    "    #y = test_array[:,-1:].copy()\n",
    "    #print('X Shape:', X.shape)\n",
    "    #print('Y Shape:', y.shape)\n",
    "    #print()\n",
    "    \n",
    "    prediction_list = []\n",
    "\n",
    "    for wf in list(reversed(range(1, walk_forward_window + 1))):\n",
    "        \n",
    "        # stop testing when we run out of target data\n",
    "        if wf < periods_ahead: break\n",
    "        \n",
    "        # get the test y\n",
    "        test_y = y[-wf+periods_ahead-1]\n",
    "\n",
    "        # create training set using scaled data\n",
    "        train_X = X[:-wf,:].copy()\n",
    "        train_y = y[:-wf].copy()\n",
    "        \n",
    "        print('Last 5 of Train X:\\n', train_X[-5:,:])\n",
    "        print('Last 5 of Train y:\\n', train_y[-5:])\n",
    "        \n",
    "        # create, scale, and reshape test set\n",
    "        if wf > periods_ahead:\n",
    "            test_X = X[:-wf+periods_ahead,:].copy()\n",
    "            test_y_acc_check = y[-wf:-wf+periods_ahead].copy()\n",
    "            \n",
    "            print('Last 5 of Test X:\\n', test_X[-5:,:])\n",
    "            print('Last 5 of Test y Acc Check:\\n', test_y_acc_check[-5:])\n",
    "            \n",
    "        elif wf == periods_ahead:\n",
    "            test_X = X.copy()\n",
    "            test_y_acc_check = y.copy()\n",
    "            \n",
    "            print('Last 5 of Test X:\\n', test_X[-5:,:])\n",
    "            print('Last 5 of Test y Acc Check:\\n', test_y_acc_check[-5:])\n",
    "\n",
    "        print('Ultimate Test y:', test_y)\n",
    "        \n",
    "        # fit model no training data\n",
    "        model = XGBClassifier(**xgb_params)\n",
    "        model.fit(train_X, train_y)\n",
    "        \n",
    "        # make predictions for test data\n",
    "        y_pred = model.predict_proba(test_X)\n",
    "        \n",
    "        print('Test X shape:', test_X.shape)\n",
    "        print('predictions shape:', y_pred.shape)\n",
    "        \n",
    "        # get the last value of the predictions (the ultimate target)\n",
    "        yhat = tuple(y_pred[-1])\n",
    "        \n",
    "        print('\\n', wf - 1, 'steps left')\n",
    "        print('predicted:', yhat)\n",
    "        print('actual:', test_y, '\\n')\n",
    "        \n",
    "        # create a target and sequence tuple of results for saving\n",
    "        tar_tuple = (test_y, yhat)\n",
    "        \n",
    "        prediction_list.append(tar_tuple)\n",
    "        if wf % 25 == 0: print(prediction_list)\n",
    "        print()\n",
    "    \n",
    "    iter_results[iter_id] = prediction_list\n",
    "    print('\\n\\n\\n')\n",
    "    print(iter_results)\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Get the Start Date for the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgboost_lagged_ens = [\n",
    "    \n",
    "    {'features': 20, 'timesteps': 24, 'max_depth': 20, 'learning_rate': 0.005, 'n_estimators': 1500}\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Date: 2017-06-12 00:00:00\n",
      "Target: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_wf_start_date(df6,\n",
    "                  param_dict = xgboost_lagged_ens[0],\n",
    "                  _target = _target,\n",
    "                  #_wfw = walk_forward_window,\n",
    "                  _wfw = 425,\n",
    "                  _periods_ahead = 4\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
