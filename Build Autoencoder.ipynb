{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# ml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (630, 712)\n",
      "Test Shape: (211, 712)\n"
     ]
    }
   ],
   "source": [
    "# load and clean\n",
    "df = pd.read_csv('_trainingData/train_full_6hr_2017-09-28.csv')\n",
    "df.index = pd.to_datetime(df.date)\n",
    "df = df.drop('date', 1)\n",
    "df = df[df.index >= datetime(2017, 3, 2)]\n",
    "\n",
    "# identify the target col (unshifted / adjusted) and filter down features / df\n",
    "unshifted_target_col = 'polo_usdteth_median_trade_price'\n",
    "cols_no_target = [col for col in df.columns.tolist() if unshifted_target_col not in col]\n",
    "\n",
    "# save the target col for later and remove from dataset\n",
    "unshifted_target_series = df[unshifted_target_col].copy()\n",
    "df = df[cols_no_target]\n",
    "\n",
    "# create matrix for X\n",
    "X = df.as_matrix().copy()\n",
    "y = unshifted_target_series.as_matrix().copy()\n",
    "\n",
    "# and scale it\n",
    "#scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "\n",
    "# get train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y)\n",
    "\n",
    "print('Train Shape:', X_train.shape)\n",
    "print('Test Shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Simple Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 630 samples, validate on 211 samples\n",
      "Epoch 1/200\n",
      "630/630 [==============================] - 0s - loss: 0.0467 - val_loss: 0.0236\n",
      "Epoch 2/200\n",
      "630/630 [==============================] - 0s - loss: 0.0199 - val_loss: 0.0173\n",
      "Epoch 3/200\n",
      "630/630 [==============================] - 0s - loss: 0.0157 - val_loss: 0.0151\n",
      "Epoch 4/200\n",
      "630/630 [==============================] - 0s - loss: 0.0141 - val_loss: 0.0141\n",
      "Epoch 5/200\n",
      "630/630 [==============================] - 0s - loss: 0.0132 - val_loss: 0.0134\n",
      "Epoch 6/200\n",
      "630/630 [==============================] - 0s - loss: 0.0126 - val_loss: 0.0129\n",
      "Epoch 7/200\n",
      "630/630 [==============================] - 0s - loss: 0.0121 - val_loss: 0.0123\n",
      "Epoch 8/200\n",
      "630/630 [==============================] - 0s - loss: 0.0115 - val_loss: 0.0119\n",
      "Epoch 9/200\n",
      "630/630 [==============================] - 0s - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 10/200\n",
      "630/630 [==============================] - 0s - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 11/200\n",
      "630/630 [==============================] - 0s - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 12/200\n",
      "630/630 [==============================] - 0s - loss: 0.0101 - val_loss: 0.0107\n",
      "Epoch 13/200\n",
      "630/630 [==============================] - 0s - loss: 0.0098 - val_loss: 0.0103\n",
      "Epoch 14/200\n",
      "630/630 [==============================] - 0s - loss: 0.0094 - val_loss: 0.0100\n",
      "Epoch 15/200\n",
      "630/630 [==============================] - 0s - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 16/200\n",
      "630/630 [==============================] - 0s - loss: 0.0089 - val_loss: 0.0095\n",
      "Epoch 17/200\n",
      "630/630 [==============================] - 0s - loss: 0.0086 - val_loss: 0.0091\n",
      "Epoch 18/200\n",
      "630/630 [==============================] - 0s - loss: 0.0084 - val_loss: 0.0089\n",
      "Epoch 19/200\n",
      "630/630 [==============================] - 0s - loss: 0.0081 - val_loss: 0.0086\n",
      "Epoch 20/200\n",
      "630/630 [==============================] - 0s - loss: 0.0079 - val_loss: 0.0084\n",
      "Epoch 21/200\n",
      "630/630 [==============================] - 0s - loss: 0.0077 - val_loss: 0.0082\n",
      "Epoch 22/200\n",
      "630/630 [==============================] - 0s - loss: 0.0075 - val_loss: 0.0081\n",
      "Epoch 23/200\n",
      "630/630 [==============================] - 0s - loss: 0.0074 - val_loss: 0.0079\n",
      "Epoch 24/200\n",
      "630/630 [==============================] - 0s - loss: 0.0072 - val_loss: 0.0077\n",
      "Epoch 25/200\n",
      "630/630 [==============================] - 0s - loss: 0.0071 - val_loss: 0.0076\n",
      "Epoch 26/200\n",
      "630/630 [==============================] - 0s - loss: 0.0069 - val_loss: 0.0075\n",
      "Epoch 27/200\n",
      "630/630 [==============================] - 0s - loss: 0.0068 - val_loss: 0.0073\n",
      "Epoch 28/200\n",
      "630/630 [==============================] - 0s - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 29/200\n",
      "630/630 [==============================] - 0s - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 30/200\n",
      "630/630 [==============================] - 0s - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 31/200\n",
      "630/630 [==============================] - 0s - loss: 0.0064 - val_loss: 0.0069\n",
      "Epoch 32/200\n",
      "630/630 [==============================] - 0s - loss: 0.0062 - val_loss: 0.0068\n",
      "Epoch 33/200\n",
      "630/630 [==============================] - 0s - loss: 0.0062 - val_loss: 0.0068\n",
      "Epoch 34/200\n",
      "630/630 [==============================] - 0s - loss: 0.0061 - val_loss: 0.0067\n",
      "Epoch 35/200\n",
      "630/630 [==============================] - 0s - loss: 0.0060 - val_loss: 0.0066\n",
      "Epoch 36/200\n",
      "630/630 [==============================] - 0s - loss: 0.0059 - val_loss: 0.0066\n",
      "Epoch 37/200\n",
      "630/630 [==============================] - 0s - loss: 0.0059 - val_loss: 0.0065\n",
      "Epoch 38/200\n",
      "630/630 [==============================] - 0s - loss: 0.0058 - val_loss: 0.0065\n",
      "Epoch 39/200\n",
      "630/630 [==============================] - 0s - loss: 0.0057 - val_loss: 0.0064\n",
      "Epoch 40/200\n",
      "630/630 [==============================] - 0s - loss: 0.0056 - val_loss: 0.0063\n",
      "Epoch 41/200\n",
      "630/630 [==============================] - 0s - loss: 0.0056 - val_loss: 0.0063\n",
      "Epoch 42/200\n",
      "630/630 [==============================] - 0s - loss: 0.0055 - val_loss: 0.0063\n",
      "Epoch 43/200\n",
      "630/630 [==============================] - 0s - loss: 0.0055 - val_loss: 0.0062\n",
      "Epoch 44/200\n",
      "630/630 [==============================] - 0s - loss: 0.0054 - val_loss: 0.0062\n",
      "Epoch 45/200\n",
      "630/630 [==============================] - 0s - loss: 0.0054 - val_loss: 0.0061\n",
      "Epoch 46/200\n",
      "630/630 [==============================] - 0s - loss: 0.0053 - val_loss: 0.0061\n",
      "Epoch 47/200\n",
      "630/630 [==============================] - 0s - loss: 0.0053 - val_loss: 0.0061\n",
      "Epoch 48/200\n",
      "630/630 [==============================] - 0s - loss: 0.0052 - val_loss: 0.0061\n",
      "Epoch 49/200\n",
      "630/630 [==============================] - 0s - loss: 0.0052 - val_loss: 0.0060\n",
      "Epoch 50/200\n",
      "630/630 [==============================] - 0s - loss: 0.0052 - val_loss: 0.0060\n",
      "Epoch 51/200\n",
      "630/630 [==============================] - 0s - loss: 0.0051 - val_loss: 0.0060\n",
      "Epoch 52/200\n",
      "630/630 [==============================] - 0s - loss: 0.0051 - val_loss: 0.0059\n",
      "Epoch 53/200\n",
      "630/630 [==============================] - 0s - loss: 0.0050 - val_loss: 0.0059\n",
      "Epoch 54/200\n",
      "630/630 [==============================] - 0s - loss: 0.0050 - val_loss: 0.0059\n",
      "Epoch 55/200\n",
      "630/630 [==============================] - 0s - loss: 0.0050 - val_loss: 0.0059\n",
      "Epoch 56/200\n",
      "630/630 [==============================] - 0s - loss: 0.0050 - val_loss: 0.0059\n",
      "Epoch 57/200\n",
      "630/630 [==============================] - 0s - loss: 0.0049 - val_loss: 0.0058\n",
      "Epoch 58/200\n",
      "630/630 [==============================] - 0s - loss: 0.0049 - val_loss: 0.0058\n",
      "Epoch 59/200\n",
      "630/630 [==============================] - 0s - loss: 0.0049 - val_loss: 0.0058\n",
      "Epoch 60/200\n",
      "630/630 [==============================] - 0s - loss: 0.0048 - val_loss: 0.0058\n",
      "Epoch 61/200\n",
      "630/630 [==============================] - 0s - loss: 0.0048 - val_loss: 0.0058\n",
      "Epoch 62/200\n",
      "630/630 [==============================] - 0s - loss: 0.0048 - val_loss: 0.0058\n",
      "Epoch 63/200\n",
      "630/630 [==============================] - 0s - loss: 0.0048 - val_loss: 0.0058\n",
      "Epoch 64/200\n",
      "630/630 [==============================] - 0s - loss: 0.0047 - val_loss: 0.0057\n",
      "Epoch 65/200\n",
      "630/630 [==============================] - 0s - loss: 0.0048 - val_loss: 0.0057\n",
      "Epoch 66/200\n",
      "630/630 [==============================] - 0s - loss: 0.0047 - val_loss: 0.0057\n",
      "Epoch 67/200\n",
      "630/630 [==============================] - 0s - loss: 0.0047 - val_loss: 0.0057\n",
      "Epoch 68/200\n",
      "630/630 [==============================] - 0s - loss: 0.0047 - val_loss: 0.0057\n",
      "Epoch 69/200\n",
      "630/630 [==============================] - 0s - loss: 0.0047 - val_loss: 0.0057\n",
      "Epoch 70/200\n",
      "630/630 [==============================] - 0s - loss: 0.0046 - val_loss: 0.0057\n",
      "Epoch 71/200\n",
      "630/630 [==============================] - 0s - loss: 0.0046 - val_loss: 0.0057\n",
      "Epoch 72/200\n",
      "630/630 [==============================] - 0s - loss: 0.0046 - val_loss: 0.0057\n",
      "Epoch 73/200\n",
      "630/630 [==============================] - 0s - loss: 0.0046 - val_loss: 0.0056\n",
      "Epoch 74/200\n",
      "630/630 [==============================] - 0s - loss: 0.0046 - val_loss: 0.0056\n",
      "Epoch 75/200\n",
      "630/630 [==============================] - 0s - loss: 0.0046 - val_loss: 0.0056\n",
      "Epoch 76/200\n",
      "630/630 [==============================] - 0s - loss: 0.0046 - val_loss: 0.0056\n",
      "Epoch 77/200\n",
      "630/630 [==============================] - 0s - loss: 0.0046 - val_loss: 0.0056\n",
      "Epoch 78/200\n",
      "630/630 [==============================] - 0s - loss: 0.0045 - val_loss: 0.0056\n",
      "Epoch 79/200\n",
      "630/630 [==============================] - 0s - loss: 0.0045 - val_loss: 0.0057\n",
      "Epoch 80/200\n",
      "630/630 [==============================] - 0s - loss: 0.0045 - val_loss: 0.0056\n",
      "Epoch 81/200\n",
      "630/630 [==============================] - 0s - loss: 0.0045 - val_loss: 0.0056\n",
      "Epoch 82/200\n",
      "630/630 [==============================] - 0s - loss: 0.0045 - val_loss: 0.0056\n",
      "Epoch 83/200\n",
      "630/630 [==============================] - 0s - loss: 0.0045 - val_loss: 0.0056\n",
      "Epoch 84/200\n",
      "630/630 [==============================] - 0s - loss: 0.0045 - val_loss: 0.0057\n",
      "Epoch 85/200\n",
      "630/630 [==============================] - 0s - loss: 0.0045 - val_loss: 0.0056\n",
      "Epoch 86/200\n",
      "630/630 [==============================] - 0s - loss: 0.0044 - val_loss: 0.0056\n",
      "Epoch 87/200\n",
      "630/630 [==============================] - 0s - loss: 0.0044 - val_loss: 0.0056\n",
      "Epoch 88/200\n",
      "630/630 [==============================] - 0s - loss: 0.0044 - val_loss: 0.0056\n",
      "Epoch 89/200\n",
      "630/630 [==============================] - 0s - loss: 0.0044 - val_loss: 0.0056\n",
      "Epoch 90/200\n",
      "630/630 [==============================] - 0s - loss: 0.0044 - val_loss: 0.0056\n",
      "Epoch 91/200\n",
      "630/630 [==============================] - 0s - loss: 0.0044 - val_loss: 0.0055\n",
      "Epoch 92/200\n",
      "630/630 [==============================] - 0s - loss: 0.0044 - val_loss: 0.0056\n",
      "Epoch 93/200\n",
      "630/630 [==============================] - 0s - loss: 0.0044 - val_loss: 0.0055\n",
      "Epoch 94/200\n",
      "630/630 [==============================] - 0s - loss: 0.0044 - val_loss: 0.0055\n",
      "Epoch 95/200\n",
      "630/630 [==============================] - 0s - loss: 0.0044 - val_loss: 0.0055\n",
      "Epoch 96/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0055\n",
      "Epoch 97/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0055\n",
      "Epoch 98/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0056\n",
      "Epoch 99/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0055\n",
      "Epoch 100/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0056\n",
      "Epoch 101/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0056\n",
      "Epoch 102/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0055\n",
      "Epoch 103/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0055\n",
      "Epoch 104/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0055\n",
      "Epoch 105/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0056\n",
      "Epoch 106/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0055\n",
      "Epoch 107/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0055\n",
      "Epoch 108/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0055\n",
      "Epoch 109/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0055\n",
      "Epoch 110/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 111/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 112/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 113/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0056\n",
      "Epoch 114/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 115/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 116/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0056\n",
      "Epoch 117/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 118/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 119/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 120/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0056\n",
      "Epoch 121/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 122/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0056\n",
      "Epoch 123/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 124/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 125/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 126/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 127/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 128/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0056\n",
      "Epoch 129/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 130/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 131/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 132/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 133/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 134/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 135/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 136/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 137/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 138/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0056\n",
      "Epoch 139/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 140/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 141/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 142/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 143/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 144/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 145/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 146/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 147/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 148/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 149/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 150/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 151/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 152/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 153/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 154/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 155/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 156/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 157/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 158/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 159/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0056\n",
      "Epoch 160/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 161/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 162/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 163/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 164/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 165/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 166/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 167/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 168/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 169/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 170/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 171/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 172/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 173/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 174/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 176/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 177/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 178/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 179/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 180/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0056\n",
      "Epoch 181/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 182/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0056\n",
      "Epoch 183/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 184/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 185/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 186/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0056\n",
      "Epoch 187/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0055\n",
      "Epoch 188/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0055\n",
      "Epoch 189/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0055\n",
      "Epoch 190/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0055\n",
      "Epoch 191/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0056\n",
      "Epoch 192/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0056\n",
      "Epoch 193/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0056\n",
      "Epoch 194/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0056\n",
      "Epoch 195/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0055\n",
      "Epoch 196/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0055\n",
      "Epoch 197/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0056\n",
      "Epoch 198/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0056\n",
      "Epoch 199/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0055\n",
      "Epoch 200/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b01c8d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is our input placeholder\n",
    "input_data = Input(shape=(df.shape[1],))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_data)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(df.shape[1], activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_data, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_data, encoded)\n",
    "\n",
    "# compile the autoencoder\n",
    "autoencoder.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# and fit it!\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs = 200,\n",
    "                batch_size = 8,\n",
    "                shuffle = True,\n",
    "                validation_data = (X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_data = encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211, 32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 16 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 211 / 211\n",
      "[t-SNE] Mean sigma: 3.119169\n",
      "[t-SNE] KL divergence after 100 iterations with early exaggeration: 1.145939\n",
      "[t-SNE] Error after 175 iterations: 1.145939\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCBR8QghpCP8frZUoxAhI9TwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1211a94e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot t-SNE projection\n",
    "tsne = TSNE(verbose = 1,\n",
    "            n_components = 2,\n",
    "            perplexity = 5,\n",
    "            n_iter = 10000,\n",
    "            learning_rate = 200\n",
    "           )\n",
    "\n",
    "projection = tsne.fit_transform(encoded_data)\n",
    "plt.scatter(*projection.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Build Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 630 samples, validate on 211 samples\n",
      "Epoch 1/100\n",
      "630/630 [==============================] - 1s - loss: 0.0466 - val_loss: 0.0250\n",
      "Epoch 2/100\n",
      "630/630 [==============================] - 0s - loss: 0.0204 - val_loss: 0.0173\n",
      "Epoch 3/100\n",
      "630/630 [==============================] - 0s - loss: 0.0160 - val_loss: 0.0141\n",
      "Epoch 4/100\n",
      "630/630 [==============================] - 0s - loss: 0.0136 - val_loss: 0.0127\n",
      "Epoch 5/100\n",
      "630/630 [==============================] - 0s - loss: 0.0126 - val_loss: 0.0120\n",
      "Epoch 6/100\n",
      "630/630 [==============================] - 0s - loss: 0.0119 - val_loss: 0.0117\n",
      "Epoch 7/100\n",
      "630/630 [==============================] - 0s - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 8/100\n",
      "630/630 [==============================] - 0s - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 9/100\n",
      "630/630 [==============================] - 0s - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 10/100\n",
      "630/630 [==============================] - 0s - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 11/100\n",
      "630/630 [==============================] - 0s - loss: 0.0099 - val_loss: 0.0099\n",
      "Epoch 12/100\n",
      "630/630 [==============================] - 0s - loss: 0.0094 - val_loss: 0.0097\n",
      "Epoch 13/100\n",
      "630/630 [==============================] - 0s - loss: 0.0090 - val_loss: 0.0092\n",
      "Epoch 14/100\n",
      "630/630 [==============================] - 0s - loss: 0.0086 - val_loss: 0.0090\n",
      "Epoch 15/100\n",
      "630/630 [==============================] - 0s - loss: 0.0083 - val_loss: 0.0085\n",
      "Epoch 16/100\n",
      "630/630 [==============================] - 0s - loss: 0.0078 - val_loss: 0.0084\n",
      "Epoch 17/100\n",
      "630/630 [==============================] - 0s - loss: 0.0076 - val_loss: 0.0081\n",
      "Epoch 18/100\n",
      "630/630 [==============================] - 0s - loss: 0.0073 - val_loss: 0.0078\n",
      "Epoch 19/100\n",
      "630/630 [==============================] - 0s - loss: 0.0071 - val_loss: 0.0076\n",
      "Epoch 20/100\n",
      "630/630 [==============================] - 0s - loss: 0.0069 - val_loss: 0.0076\n",
      "Epoch 21/100\n",
      "630/630 [==============================] - 0s - loss: 0.0068 - val_loss: 0.0075\n",
      "Epoch 22/100\n",
      "630/630 [==============================] - 0s - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 23/100\n",
      "630/630 [==============================] - 0s - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 24/100\n",
      "630/630 [==============================] - 0s - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 25/100\n",
      "630/630 [==============================] - 0s - loss: 0.0062 - val_loss: 0.0070\n",
      "Epoch 26/100\n",
      "630/630 [==============================] - 0s - loss: 0.0061 - val_loss: 0.0071\n",
      "Epoch 27/100\n",
      "630/630 [==============================] - 0s - loss: 0.0061 - val_loss: 0.0069\n",
      "Epoch 28/100\n",
      "630/630 [==============================] - 0s - loss: 0.0059 - val_loss: 0.0069\n",
      "Epoch 29/100\n",
      "630/630 [==============================] - 0s - loss: 0.0058 - val_loss: 0.0069\n",
      "Epoch 30/100\n",
      "630/630 [==============================] - 0s - loss: 0.0058 - val_loss: 0.0067\n",
      "Epoch 31/100\n",
      "630/630 [==============================] - 0s - loss: 0.0056 - val_loss: 0.0068\n",
      "Epoch 32/100\n",
      "630/630 [==============================] - 0s - loss: 0.0056 - val_loss: 0.0066\n",
      "Epoch 33/100\n",
      "630/630 [==============================] - 0s - loss: 0.0054 - val_loss: 0.0066\n",
      "Epoch 34/100\n",
      "630/630 [==============================] - 0s - loss: 0.0053 - val_loss: 0.0065\n",
      "Epoch 35/100\n",
      "630/630 [==============================] - 0s - loss: 0.0052 - val_loss: 0.0065\n",
      "Epoch 36/100\n",
      "630/630 [==============================] - 0s - loss: 0.0052 - val_loss: 0.0064\n",
      "Epoch 37/100\n",
      "630/630 [==============================] - 0s - loss: 0.0051 - val_loss: 0.0065\n",
      "Epoch 38/100\n",
      "630/630 [==============================] - 0s - loss: 0.0050 - val_loss: 0.0064\n",
      "Epoch 39/100\n",
      "630/630 [==============================] - 0s - loss: 0.0050 - val_loss: 0.0065\n",
      "Epoch 40/100\n",
      "630/630 [==============================] - 0s - loss: 0.0050 - val_loss: 0.0063\n",
      "Epoch 41/100\n",
      "630/630 [==============================] - 0s - loss: 0.0050 - val_loss: 0.0064\n",
      "Epoch 42/100\n",
      "630/630 [==============================] - ETA: 0s - loss: 0.004 - 0s - loss: 0.0048 - val_loss: 0.0062\n",
      "Epoch 43/100\n",
      "630/630 [==============================] - 0s - loss: 0.0047 - val_loss: 0.0062\n",
      "Epoch 44/100\n",
      "630/630 [==============================] - 0s - loss: 0.0047 - val_loss: 0.0062\n",
      "Epoch 45/100\n",
      "630/630 [==============================] - 0s - loss: 0.0046 - val_loss: 0.0061\n",
      "Epoch 46/100\n",
      "630/630 [==============================] - 0s - loss: 0.0046 - val_loss: 0.0061\n",
      "Epoch 47/100\n",
      "630/630 [==============================] - 0s - loss: 0.0045 - val_loss: 0.0061\n",
      "Epoch 48/100\n",
      "630/630 [==============================] - 0s - loss: 0.0044 - val_loss: 0.0062\n",
      "Epoch 49/100\n",
      "630/630 [==============================] - 0s - loss: 0.0044 - val_loss: 0.0060\n",
      "Epoch 50/100\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0060\n",
      "Epoch 51/100\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0061\n",
      "Epoch 52/100\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0060\n",
      "Epoch 53/100\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0060\n",
      "Epoch 54/100\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0060\n",
      "Epoch 55/100\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0060\n",
      "Epoch 56/100\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0059\n",
      "Epoch 57/100\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0060\n",
      "Epoch 58/100\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0059\n",
      "Epoch 59/100\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0059\n",
      "Epoch 60/100\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0059\n",
      "Epoch 61/100\n",
      "630/630 [==============================] - 0s - loss: 0.0038 - val_loss: 0.0058\n",
      "Epoch 62/100\n",
      "630/630 [==============================] - 0s - loss: 0.0038 - val_loss: 0.0057\n",
      "Epoch 63/100\n",
      "630/630 [==============================] - 0s - loss: 0.0037 - val_loss: 0.0058\n",
      "Epoch 64/100\n",
      "630/630 [==============================] - 0s - loss: 0.0038 - val_loss: 0.0059\n",
      "Epoch 65/100\n",
      "630/630 [==============================] - 0s - loss: 0.0038 - val_loss: 0.0058\n",
      "Epoch 66/100\n",
      "630/630 [==============================] - 0s - loss: 0.0036 - val_loss: 0.0058\n",
      "Epoch 67/100\n",
      "630/630 [==============================] - 0s - loss: 0.0036 - val_loss: 0.0057\n",
      "Epoch 68/100\n",
      "630/630 [==============================] - 0s - loss: 0.0036 - val_loss: 0.0058\n",
      "Epoch 69/100\n",
      "630/630 [==============================] - 0s - loss: 0.0036 - val_loss: 0.0058\n",
      "Epoch 70/100\n",
      "630/630 [==============================] - 0s - loss: 0.0036 - val_loss: 0.0057\n",
      "Epoch 71/100\n",
      "630/630 [==============================] - 0s - loss: 0.0036 - val_loss: 0.0058\n",
      "Epoch 72/100\n",
      "630/630 [==============================] - 0s - loss: 0.0035 - val_loss: 0.0056\n",
      "Epoch 73/100\n",
      "630/630 [==============================] - 0s - loss: 0.0034 - val_loss: 0.0056\n",
      "Epoch 74/100\n",
      "630/630 [==============================] - 0s - loss: 0.0034 - val_loss: 0.0057\n",
      "Epoch 75/100\n",
      "630/630 [==============================] - 0s - loss: 0.0034 - val_loss: 0.0057\n",
      "Epoch 76/100\n",
      "630/630 [==============================] - 0s - loss: 0.0034 - val_loss: 0.0056\n",
      "Epoch 77/100\n",
      "630/630 [==============================] - 0s - loss: 0.0033 - val_loss: 0.0056\n",
      "Epoch 78/100\n",
      "630/630 [==============================] - 0s - loss: 0.0033 - val_loss: 0.0056\n",
      "Epoch 79/100\n",
      "630/630 [==============================] - 0s - loss: 0.0032 - val_loss: 0.0057\n",
      "Epoch 80/100\n",
      "630/630 [==============================] - 0s - loss: 0.0033 - val_loss: 0.0056\n",
      "Epoch 81/100\n",
      "630/630 [==============================] - 0s - loss: 0.0033 - val_loss: 0.0059\n",
      "Epoch 82/100\n",
      "630/630 [==============================] - 0s - loss: 0.0032 - val_loss: 0.0057\n",
      "Epoch 83/100\n",
      "630/630 [==============================] - 0s - loss: 0.0032 - val_loss: 0.0056\n",
      "Epoch 84/100\n",
      "630/630 [==============================] - 0s - loss: 0.0033 - val_loss: 0.0056\n",
      "Epoch 85/100\n",
      "630/630 [==============================] - 0s - loss: 0.0032 - val_loss: 0.0057\n",
      "Epoch 86/100\n",
      "630/630 [==============================] - 0s - loss: 0.0031 - val_loss: 0.0055\n",
      "Epoch 87/100\n",
      "630/630 [==============================] - 0s - loss: 0.0031 - val_loss: 0.0055\n",
      "Epoch 88/100\n",
      "630/630 [==============================] - 0s - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 89/100\n",
      "630/630 [==============================] - 0s - loss: 0.0030 - val_loss: 0.0055\n",
      "Epoch 90/100\n",
      "630/630 [==============================] - 0s - loss: 0.0030 - val_loss: 0.0055\n",
      "Epoch 91/100\n",
      "630/630 [==============================] - 0s - loss: 0.0030 - val_loss: 0.0056\n",
      "Epoch 92/100\n",
      "630/630 [==============================] - 0s - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 93/100\n",
      "630/630 [==============================] - 0s - loss: 0.0029 - val_loss: 0.0055\n",
      "Epoch 94/100\n",
      "630/630 [==============================] - 0s - loss: 0.0029 - val_loss: 0.0055\n",
      "Epoch 95/100\n",
      "630/630 [==============================] - 0s - loss: 0.0029 - val_loss: 0.0055\n",
      "Epoch 96/100\n",
      "630/630 [==============================] - 0s - loss: 0.0029 - val_loss: 0.0055\n",
      "Epoch 97/100\n",
      "630/630 [==============================] - 0s - loss: 0.0029 - val_loss: 0.0055\n",
      "Epoch 98/100\n",
      "630/630 [==============================] - 0s - loss: 0.0029 - val_loss: 0.0056\n",
      "Epoch 99/100\n",
      "630/630 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0055\n",
      "Epoch 100/100\n",
      "630/630 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1292adda0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = Input(shape=(df.shape[1],))\n",
    "\n",
    "# encoding layers\n",
    "encoded = Dense(256, activation='relu')(input_data)\n",
    "encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "\n",
    "# decoding layers\n",
    "decoded = Dense(128, activation='relu')(encoded)\n",
    "decoded = Dense(256, activation='relu')(decoded)\n",
    "decoded = Dense(df.shape[1], activation='sigmoid')(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_data, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_data, encoded)\n",
    "\n",
    "# compile the autoencoder\n",
    "autoencoder.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# and fit it!\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs = 100,\n",
    "                batch_size = 16,\n",
    "                shuffle = True,\n",
    "                validation_data = (X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Non-Zero Features: 48\n"
     ]
    }
   ],
   "source": [
    "encoded_data = encoder.predict(X_test)\n",
    "print('Number of Non-Zero Features:', sum(pd.DataFrame(encoded_data).sum() > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.073737228300170307"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.evaluate(X_test, X_test, verbose=0) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 210 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 211 / 211\n",
      "[t-SNE] Mean sigma: 6.589547\n",
      "[t-SNE] KL divergence after 75 iterations with early exaggeration: 0.198969\n",
      "[t-SNE] Error after 100 iterations: 0.198969\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npGX8f607iq7I3jmXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a98a898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot t-SNE projection\n",
    "tsne = TSNE(verbose = 1,\n",
    "            n_components = 2,\n",
    "            perplexity = 80,\n",
    "            n_iter = 10000,\n",
    "            learning_rate = 200\n",
    "           )\n",
    "\n",
    "projection = tsne.fit_transform(encoded_data)\n",
    "plt.scatter(*projection.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.669755</td>\n",
       "      <td>2.169203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.592996</td>\n",
       "      <td>2.672249</td>\n",
       "      <td>2.633171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.193692</td>\n",
       "      <td>4.700568</td>\n",
       "      <td>...</td>\n",
       "      <td>4.487885</td>\n",
       "      <td>3.574825</td>\n",
       "      <td>1.942775</td>\n",
       "      <td>2.469156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.002975</td>\n",
       "      <td>3.108800</td>\n",
       "      <td>3.662721</td>\n",
       "      <td>2.820835</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.260590</td>\n",
       "      <td>6.828120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.102756</td>\n",
       "      <td>2.839905</td>\n",
       "      <td>2.291757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.367377</td>\n",
       "      <td>0.097097</td>\n",
       "      <td>...</td>\n",
       "      <td>2.810404</td>\n",
       "      <td>5.571050</td>\n",
       "      <td>3.839996</td>\n",
       "      <td>3.807738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.874125</td>\n",
       "      <td>6.373225</td>\n",
       "      <td>4.372602</td>\n",
       "      <td>0.175591</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.010005</td>\n",
       "      <td>7.900447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.912382</td>\n",
       "      <td>0.272558</td>\n",
       "      <td>2.441691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.020359</td>\n",
       "      <td>1.735810</td>\n",
       "      <td>...</td>\n",
       "      <td>2.286294</td>\n",
       "      <td>2.664440</td>\n",
       "      <td>4.849371</td>\n",
       "      <td>0.163668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.310873</td>\n",
       "      <td>4.022952</td>\n",
       "      <td>0.606387</td>\n",
       "      <td>0.893332</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.488494</td>\n",
       "      <td>4.137077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.202023</td>\n",
       "      <td>0.898835</td>\n",
       "      <td>3.581718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.787268</td>\n",
       "      <td>6.496127</td>\n",
       "      <td>...</td>\n",
       "      <td>1.853271</td>\n",
       "      <td>2.327532</td>\n",
       "      <td>0.610046</td>\n",
       "      <td>2.793788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.635255</td>\n",
       "      <td>1.294477</td>\n",
       "      <td>2.346711</td>\n",
       "      <td>3.285391</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.765625</td>\n",
       "      <td>6.359138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.390718</td>\n",
       "      <td>2.897318</td>\n",
       "      <td>1.989465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.625733</td>\n",
       "      <td>1.463922</td>\n",
       "      <td>...</td>\n",
       "      <td>6.084335</td>\n",
       "      <td>3.719874</td>\n",
       "      <td>4.985308</td>\n",
       "      <td>4.419353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.476301</td>\n",
       "      <td>4.816261</td>\n",
       "      <td>3.301551</td>\n",
       "      <td>2.669817</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2    3         4         5         6    7         8   \\\n",
       "0  0.0  4.669755  2.169203  0.0  4.592996  2.672249  2.633171  0.0  3.193692   \n",
       "1  0.0  3.260590  6.828120  0.0  2.102756  2.839905  2.291757  0.0  2.367377   \n",
       "2  0.0  4.010005  7.900447  0.0  1.912382  0.272558  2.441691  0.0  1.020359   \n",
       "3  0.0  5.488494  4.137077  0.0  3.202023  0.898835  3.581718  0.0  1.787268   \n",
       "4  0.0  2.765625  6.359138  0.0  1.390718  2.897318  1.989465  0.0  1.625733   \n",
       "\n",
       "         9  ...         54        55        56        57   58        59  \\\n",
       "0  4.700568 ...   4.487885  3.574825  1.942775  2.469156  0.0  4.002975   \n",
       "1  0.097097 ...   2.810404  5.571050  3.839996  3.807738  0.0  2.874125   \n",
       "2  1.735810 ...   2.286294  2.664440  4.849371  0.163668  0.0  5.310873   \n",
       "3  6.496127 ...   1.853271  2.327532  0.610046  2.793788  0.0  5.635255   \n",
       "4  1.463922 ...   6.084335  3.719874  4.985308  4.419353  0.0  3.476301   \n",
       "\n",
       "         60        61        62   63  \n",
       "0  3.108800  3.662721  2.820835  0.0  \n",
       "1  6.373225  4.372602  0.175591  0.0  \n",
       "2  4.022952  0.606387  0.893332  0.0  \n",
       "3  1.294477  2.346711  3.285391  0.0  \n",
       "4  4.816261  3.301551  2.669817  0.0  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(encoded_data).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Pattern Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (630, 529)\n",
      "Test Shape: (211, 529)\n"
     ]
    }
   ],
   "source": [
    "# load and clean\n",
    "df = pd.read_csv('_trainingData/train_full_6hr_2017-09-28.csv')\n",
    "df.index = pd.to_datetime(df.date)\n",
    "df = df.drop('date', 1)\n",
    "df = df[df.index >= datetime(2017, 3, 2)]\n",
    "\n",
    "# identify the target col (unshifted / adjusted) and filter down features / df\n",
    "unshifted_target_col = 'polo_usdteth_median_trade_price'\n",
    "cols_no_target = [col for col in df.columns.tolist() if unshifted_target_col not in col and '_pat_' not in col]\n",
    "\n",
    "# save the target col for later and remove from dataset\n",
    "unshifted_target_series = df[unshifted_target_col].copy()\n",
    "df = df[cols_no_target]\n",
    "\n",
    "# create matrix for X\n",
    "X = df.as_matrix().copy()\n",
    "y = unshifted_target_series.as_matrix().copy()\n",
    "\n",
    "# and scale it\n",
    "#scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "\n",
    "# get train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y)\n",
    "\n",
    "print('Train Shape:', X_train.shape)\n",
    "print('Test Shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 630 samples, validate on 211 samples\n",
      "Epoch 1/200\n",
      "630/630 [==============================] - 1s - loss: 0.0591 - val_loss: 0.0350\n",
      "Epoch 2/200\n",
      "630/630 [==============================] - 0s - loss: 0.0319 - val_loss: 0.0270\n",
      "Epoch 3/200\n",
      "630/630 [==============================] - 0s - loss: 0.0221 - val_loss: 0.0211\n",
      "Epoch 4/200\n",
      "630/630 [==============================] - 0s - loss: 0.0191 - val_loss: 0.0192\n",
      "Epoch 5/200\n",
      "630/630 [==============================] - 0s - loss: 0.0173 - val_loss: 0.0168\n",
      "Epoch 6/200\n",
      "630/630 [==============================] - 0s - loss: 0.0152 - val_loss: 0.0147\n",
      "Epoch 7/200\n",
      "630/630 [==============================] - 0s - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 8/200\n",
      "630/630 [==============================] - 0s - loss: 0.0132 - val_loss: 0.0133\n",
      "Epoch 9/200\n",
      "630/630 [==============================] - 0s - loss: 0.0126 - val_loss: 0.0125\n",
      "Epoch 10/200\n",
      "630/630 [==============================] - 0s - loss: 0.0121 - val_loss: 0.0123\n",
      "Epoch 11/200\n",
      "630/630 [==============================] - 0s - loss: 0.0117 - val_loss: 0.0120\n",
      "Epoch 12/200\n",
      "630/630 [==============================] - 0s - loss: 0.0114 - val_loss: 0.0118\n",
      "Epoch 13/200\n",
      "630/630 [==============================] - 0s - loss: 0.0112 - val_loss: 0.0115\n",
      "Epoch 14/200\n",
      "630/630 [==============================] - 0s - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 15/200\n",
      "630/630 [==============================] - 0s - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 16/200\n",
      "630/630 [==============================] - 0s - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 17/200\n",
      "630/630 [==============================] - 0s - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 18/200\n",
      "630/630 [==============================] - 0s - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 19/200\n",
      "630/630 [==============================] - 0s - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 20/200\n",
      "630/630 [==============================] - 0s - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 21/200\n",
      "630/630 [==============================] - 0s - loss: 0.0096 - val_loss: 0.0102\n",
      "Epoch 22/200\n",
      "630/630 [==============================] - 0s - loss: 0.0095 - val_loss: 0.0100\n",
      "Epoch 23/200\n",
      "630/630 [==============================] - 0s - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 24/200\n",
      "630/630 [==============================] - 0s - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 25/200\n",
      "630/630 [==============================] - 0s - loss: 0.0089 - val_loss: 0.0095\n",
      "Epoch 26/200\n",
      "630/630 [==============================] - ETA: 0s - loss: 0.008 - 0s - loss: 0.0087 - val_loss: 0.0097\n",
      "Epoch 27/200\n",
      "630/630 [==============================] - 0s - loss: 0.0085 - val_loss: 0.0094\n",
      "Epoch 28/200\n",
      "630/630 [==============================] - 0s - loss: 0.0084 - val_loss: 0.0093\n",
      "Epoch 29/200\n",
      "630/630 [==============================] - 0s - loss: 0.0083 - val_loss: 0.0092\n",
      "Epoch 30/200\n",
      "630/630 [==============================] - 0s - loss: 0.0081 - val_loss: 0.0091\n",
      "Epoch 31/200\n",
      "630/630 [==============================] - 0s - loss: 0.0082 - val_loss: 0.0099\n",
      "Epoch 32/200\n",
      "630/630 [==============================] - 0s - loss: 0.0082 - val_loss: 0.0091\n",
      "Epoch 33/200\n",
      "630/630 [==============================] - 0s - loss: 0.0080 - val_loss: 0.0088\n",
      "Epoch 34/200\n",
      "630/630 [==============================] - 0s - loss: 0.0079 - val_loss: 0.0088\n",
      "Epoch 35/200\n",
      "630/630 [==============================] - 0s - loss: 0.0077 - val_loss: 0.0087\n",
      "Epoch 36/200\n",
      "630/630 [==============================] - 0s - loss: 0.0076 - val_loss: 0.0086\n",
      "Epoch 37/200\n",
      "630/630 [==============================] - 0s - loss: 0.0075 - val_loss: 0.0085\n",
      "Epoch 38/200\n",
      "630/630 [==============================] - 0s - loss: 0.0075 - val_loss: 0.0084\n",
      "Epoch 39/200\n",
      "630/630 [==============================] - 0s - loss: 0.0074 - val_loss: 0.0086\n",
      "Epoch 40/200\n",
      "630/630 [==============================] - 0s - loss: 0.0072 - val_loss: 0.0084\n",
      "Epoch 41/200\n",
      "630/630 [==============================] - 0s - loss: 0.0071 - val_loss: 0.0080\n",
      "Epoch 42/200\n",
      "630/630 [==============================] - 0s - loss: 0.0068 - val_loss: 0.0078\n",
      "Epoch 43/200\n",
      "630/630 [==============================] - 0s - loss: 0.0067 - val_loss: 0.0077\n",
      "Epoch 44/200\n",
      "630/630 [==============================] - 0s - loss: 0.0065 - val_loss: 0.0077\n",
      "Epoch 45/200\n",
      "630/630 [==============================] - 0s - loss: 0.0065 - val_loss: 0.0076\n",
      "Epoch 46/200\n",
      "630/630 [==============================] - 0s - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 47/200\n",
      "630/630 [==============================] - 0s - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 48/200\n",
      "630/630 [==============================] - 0s - loss: 0.0061 - val_loss: 0.0073\n",
      "Epoch 49/200\n",
      "630/630 [==============================] - 0s - loss: 0.0060 - val_loss: 0.0072\n",
      "Epoch 50/200\n",
      "630/630 [==============================] - 0s - loss: 0.0060 - val_loss: 0.0072\n",
      "Epoch 51/200\n",
      "630/630 [==============================] - 0s - loss: 0.0059 - val_loss: 0.0074ss: 0.0\n",
      "Epoch 52/200\n",
      "630/630 [==============================] - 0s - loss: 0.0060 - val_loss: 0.0070\n",
      "Epoch 53/200\n",
      "630/630 [==============================] - 0s - loss: 0.0058 - val_loss: 0.0070\n",
      "Epoch 54/200\n",
      "630/630 [==============================] - 0s - loss: 0.0056 - val_loss: 0.0068\n",
      "Epoch 55/200\n",
      "630/630 [==============================] - 0s - loss: 0.0055 - val_loss: 0.0067\n",
      "Epoch 56/200\n",
      "630/630 [==============================] - 0s - loss: 0.0054 - val_loss: 0.0068\n",
      "Epoch 57/200\n",
      "630/630 [==============================] - 0s - loss: 0.0054 - val_loss: 0.0067\n",
      "Epoch 58/200\n",
      "630/630 [==============================] - 0s - loss: 0.0053 - val_loss: 0.0066\n",
      "Epoch 59/200\n",
      "630/630 [==============================] - 0s - loss: 0.0052 - val_loss: 0.0066\n",
      "Epoch 60/200\n",
      "630/630 [==============================] - 0s - loss: 0.0051 - val_loss: 0.0066\n",
      "Epoch 61/200\n",
      "630/630 [==============================] - 0s - loss: 0.0052 - val_loss: 0.0065\n",
      "Epoch 62/200\n",
      "630/630 [==============================] - 0s - loss: 0.0050 - val_loss: 0.0064ss: 0.\n",
      "Epoch 63/200\n",
      "630/630 [==============================] - 0s - loss: 0.0050 - val_loss: 0.0065\n",
      "Epoch 64/200\n",
      "630/630 [==============================] - 0s - loss: 0.0049 - val_loss: 0.0063\n",
      "Epoch 65/200\n",
      "630/630 [==============================] - 0s - loss: 0.0049 - val_loss: 0.0063\n",
      "Epoch 66/200\n",
      "630/630 [==============================] - 0s - loss: 0.0049 - val_loss: 0.0064\n",
      "Epoch 67/200\n",
      "630/630 [==============================] - 0s - loss: 0.0048 - val_loss: 0.0063\n",
      "Epoch 68/200\n",
      "630/630 [==============================] - 0s - loss: 0.0047 - val_loss: 0.0062\n",
      "Epoch 69/200\n",
      "630/630 [==============================] - 0s - loss: 0.0047 - val_loss: 0.0061\n",
      "Epoch 70/200\n",
      "630/630 [==============================] - 0s - loss: 0.0046 - val_loss: 0.0061\n",
      "Epoch 71/200\n",
      "630/630 [==============================] - 0s - loss: 0.0045 - val_loss: 0.0061\n",
      "Epoch 72/200\n",
      "630/630 [==============================] - 0s - loss: 0.0046 - val_loss: 0.0061\n",
      "Epoch 73/200\n",
      "630/630 [==============================] - 0s - loss: 0.0045 - val_loss: 0.0060\n",
      "Epoch 74/200\n",
      "630/630 [==============================] - 0s - loss: 0.0045 - val_loss: 0.0061\n",
      "Epoch 75/200\n",
      "630/630 [==============================] - 0s - loss: 0.0045 - val_loss: 0.0060\n",
      "Epoch 76/200\n",
      "630/630 [==============================] - 0s - loss: 0.0045 - val_loss: 0.0061\n",
      "Epoch 77/200\n",
      "630/630 [==============================] - 0s - loss: 0.0044 - val_loss: 0.0059\n",
      "Epoch 78/200\n",
      "630/630 [==============================] - 0s - loss: 0.0044 - val_loss: 0.0060\n",
      "Epoch 79/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0059\n",
      "Epoch 80/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0060\n",
      "Epoch 81/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0059\n",
      "Epoch 82/200\n",
      "630/630 [==============================] - 0s - loss: 0.0043 - val_loss: 0.0059\n",
      "Epoch 83/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0058\n",
      "Epoch 84/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0059\n",
      "Epoch 85/200\n",
      "630/630 [==============================] - 0s - loss: 0.0042 - val_loss: 0.0058\n",
      "Epoch 86/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0058\n",
      "Epoch 87/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0058\n",
      "Epoch 88/200\n",
      "630/630 [==============================] - 0s - loss: 0.0041 - val_loss: 0.0058\n",
      "Epoch 89/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0058\n",
      "Epoch 90/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0059\n",
      "Epoch 91/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0058\n",
      "Epoch 92/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0058\n",
      "Epoch 93/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0059\n",
      "Epoch 94/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0059\n",
      "Epoch 95/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0058\n",
      "Epoch 96/200\n",
      "630/630 [==============================] - 0s - loss: 0.0040 - val_loss: 0.0057\n",
      "Epoch 97/200\n",
      "630/630 [==============================] - 0s - loss: 0.0039 - val_loss: 0.0057\n",
      "Epoch 98/200\n",
      "630/630 [==============================] - 0s - loss: 0.0038 - val_loss: 0.0057\n",
      "Epoch 99/200\n",
      "630/630 [==============================] - 0s - loss: 0.0038 - val_loss: 0.0057\n",
      "Epoch 100/200\n",
      "630/630 [==============================] - 0s - loss: 0.0038 - val_loss: 0.0058\n",
      "Epoch 101/200\n",
      "630/630 [==============================] - 0s - loss: 0.0037 - val_loss: 0.0056\n",
      "Epoch 102/200\n",
      "630/630 [==============================] - 0s - loss: 0.0037 - val_loss: 0.0056\n",
      "Epoch 103/200\n",
      "630/630 [==============================] - 0s - loss: 0.0037 - val_loss: 0.0056\n",
      "Epoch 104/200\n",
      "630/630 [==============================] - 0s - loss: 0.0036 - val_loss: 0.0056\n",
      "Epoch 105/200\n",
      "630/630 [==============================] - 0s - loss: 0.0036 - val_loss: 0.0057\n",
      "Epoch 106/200\n",
      "630/630 [==============================] - 0s - loss: 0.0036 - val_loss: 0.0056\n",
      "Epoch 107/200\n",
      "630/630 [==============================] - 0s - loss: 0.0035 - val_loss: 0.0055\n",
      "Epoch 108/200\n",
      "630/630 [==============================] - 0s - loss: 0.0036 - val_loss: 0.0056\n",
      "Epoch 109/200\n",
      "630/630 [==============================] - 0s - loss: 0.0036 - val_loss: 0.0055\n",
      "Epoch 110/200\n",
      "630/630 [==============================] - 0s - loss: 0.0036 - val_loss: 0.0056\n",
      "Epoch 111/200\n",
      "630/630 [==============================] - 0s - loss: 0.0036 - val_loss: 0.0055\n",
      "Epoch 112/200\n",
      "630/630 [==============================] - 0s - loss: 0.0036 - val_loss: 0.0055\n",
      "Epoch 113/200\n",
      "630/630 [==============================] - 0s - loss: 0.0035 - val_loss: 0.0057ss: 0\n",
      "Epoch 114/200\n",
      "630/630 [==============================] - 0s - loss: 0.0035 - val_loss: 0.0055\n",
      "Epoch 115/200\n",
      "630/630 [==============================] - 0s - loss: 0.0034 - val_loss: 0.0057\n",
      "Epoch 116/200\n",
      "630/630 [==============================] - 0s - loss: 0.0035 - val_loss: 0.0056\n",
      "Epoch 117/200\n",
      "630/630 [==============================] - 0s - loss: 0.0034 - val_loss: 0.0056\n",
      "Epoch 118/200\n",
      "630/630 [==============================] - 0s - loss: 0.0035 - val_loss: 0.0057\n",
      "Epoch 119/200\n",
      "630/630 [==============================] - 0s - loss: 0.0034 - val_loss: 0.0055\n",
      "Epoch 120/200\n",
      "630/630 [==============================] - 0s - loss: 0.0033 - val_loss: 0.0055\n",
      "Epoch 121/200\n",
      "630/630 [==============================] - 0s - loss: 0.0033 - val_loss: 0.0054ss: 0.00\n",
      "Epoch 122/200\n",
      "630/630 [==============================] - 0s - loss: 0.0032 - val_loss: 0.0054ss: 0\n",
      "Epoch 123/200\n",
      "630/630 [==============================] - 0s - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 124/200\n",
      "630/630 [==============================] - 0s - loss: 0.0034 - val_loss: 0.0055\n",
      "Epoch 125/200\n",
      "630/630 [==============================] - 0s - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 126/200\n",
      "630/630 [==============================] - 0s - loss: 0.0032 - val_loss: 0.0055\n",
      "Epoch 127/200\n",
      "630/630 [==============================] - 0s - loss: 0.0032 - val_loss: 0.0056\n",
      "Epoch 128/200\n",
      "630/630 [==============================] - 0s - loss: 0.0033 - val_loss: 0.0055\n",
      "Epoch 129/200\n",
      "630/630 [==============================] - 0s - loss: 0.0032 - val_loss: 0.0055\n",
      "Epoch 130/200\n",
      "630/630 [==============================] - 0s - loss: 0.0032 - val_loss: 0.0055\n",
      "Epoch 131/200\n",
      "630/630 [==============================] - 0s - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 132/200\n",
      "630/630 [==============================] - 0s - loss: 0.0031 - val_loss: 0.0053\n",
      "Epoch 133/200\n",
      "630/630 [==============================] - 0s - loss: 0.0031 - val_loss: 0.0053\n",
      "Epoch 134/200\n",
      "630/630 [==============================] - 0s - loss: 0.0030 - val_loss: 0.0055\n",
      "Epoch 135/200\n",
      "630/630 [==============================] - 0s - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 136/200\n",
      "630/630 [==============================] - 0s - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 137/200\n",
      "630/630 [==============================] - 0s - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 138/200\n",
      "630/630 [==============================] - 0s - loss: 0.0031 - val_loss: 0.0055\n",
      "Epoch 139/200\n",
      "630/630 [==============================] - 0s - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 140/200\n",
      "630/630 [==============================] - 0s - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 141/200\n",
      "630/630 [==============================] - 0s - loss: 0.0031 - val_loss: 0.0055\n",
      "Epoch 142/200\n",
      "630/630 [==============================] - 0s - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 143/200\n",
      "630/630 [==============================] - 0s - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 144/200\n",
      "630/630 [==============================] - 0s - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 145/200\n",
      "630/630 [==============================] - 0s - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 146/200\n",
      "630/630 [==============================] - 0s - loss: 0.0030 - val_loss: 0.0053ss: 0.\n",
      "Epoch 147/200\n",
      "630/630 [==============================] - 0s - loss: 0.0029 - val_loss: 0.0053\n",
      "Epoch 148/200\n",
      "630/630 [==============================] - 0s - loss: 0.0030 - val_loss: 0.0053\n",
      "Epoch 149/200\n",
      "630/630 [==============================] - 0s - loss: 0.0029 - val_loss: 0.0053\n",
      "Epoch 150/200\n",
      "630/630 [==============================] - 0s - loss: 0.0029 - val_loss: 0.0053\n",
      "Epoch 151/200\n",
      "630/630 [==============================] - 0s - loss: 0.0029 - val_loss: 0.0053\n",
      "Epoch 152/200\n",
      "630/630 [==============================] - 0s - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 153/200\n",
      "630/630 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0053ss: 0.\n",
      "Epoch 154/200\n",
      "630/630 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0053\n",
      "Epoch 155/200\n",
      "630/630 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0053\n",
      "Epoch 156/200\n",
      "630/630 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0053\n",
      "Epoch 157/200\n",
      "630/630 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0053\n",
      "Epoch 158/200\n",
      "630/630 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0053\n",
      "Epoch 159/200\n",
      "630/630 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0053\n",
      "Epoch 160/200\n",
      "630/630 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 161/200\n",
      "630/630 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0053\n",
      "Epoch 162/200\n",
      "630/630 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0053\n",
      "Epoch 163/200\n",
      "630/630 [==============================] - 0s - loss: 0.0027 - val_loss: 0.0053\n",
      "Epoch 164/200\n",
      "630/630 [==============================] - 0s - loss: 0.0027 - val_loss: 0.0052\n",
      "Epoch 165/200\n",
      "630/630 [==============================] - 0s - loss: 0.0027 - val_loss: 0.0051\n",
      "Epoch 166/200\n",
      "630/630 [==============================] - 0s - loss: 0.0027 - val_loss: 0.0052\n",
      "Epoch 167/200\n",
      "630/630 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0052\n",
      "Epoch 168/200\n",
      "630/630 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0053\n",
      "Epoch 169/200\n",
      "630/630 [==============================] - 0s - loss: 0.0027 - val_loss: 0.0052\n",
      "Epoch 170/200\n",
      "630/630 [==============================] - 0s - loss: 0.0027 - val_loss: 0.0051\n",
      "Epoch 171/200\n",
      "630/630 [==============================] - 0s - loss: 0.0026 - val_loss: 0.0052\n",
      "Epoch 172/200\n",
      "630/630 [==============================] - 0s - loss: 0.0026 - val_loss: 0.0052\n",
      "Epoch 173/200\n",
      "630/630 [==============================] - 0s - loss: 0.0027 - val_loss: 0.0053\n",
      "Epoch 174/200\n",
      "630/630 [==============================] - 0s - loss: 0.0027 - val_loss: 0.0052\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630/630 [==============================] - 0s - loss: 0.0026 - val_loss: 0.0051\n",
      "Epoch 176/200\n",
      "630/630 [==============================] - 0s - loss: 0.0026 - val_loss: 0.0052\n",
      "Epoch 177/200\n",
      "630/630 [==============================] - 0s - loss: 0.0026 - val_loss: 0.0051\n",
      "Epoch 178/200\n",
      "630/630 [==============================] - 0s - loss: 0.0026 - val_loss: 0.0051ss: 0.\n",
      "Epoch 179/200\n",
      "630/630 [==============================] - 0s - loss: 0.0028 - val_loss: 0.0052\n",
      "Epoch 180/200\n",
      "630/630 [==============================] - 0s - loss: 0.0027 - val_loss: 0.0052\n",
      "Epoch 181/200\n",
      "630/630 [==============================] - 0s - loss: 0.0026 - val_loss: 0.0053\n",
      "Epoch 182/200\n",
      "630/630 [==============================] - 0s - loss: 0.0026 - val_loss: 0.0053\n",
      "Epoch 183/200\n",
      "630/630 [==============================] - 0s - loss: 0.0026 - val_loss: 0.0053\n",
      "Epoch 184/200\n",
      "630/630 [==============================] - 0s - loss: 0.0026 - val_loss: 0.0052\n",
      "Epoch 185/200\n",
      "630/630 [==============================] - 0s - loss: 0.0025 - val_loss: 0.0052\n",
      "Epoch 186/200\n",
      "630/630 [==============================] - 0s - loss: 0.0025 - val_loss: 0.0051\n",
      "Epoch 187/200\n",
      "630/630 [==============================] - 0s - loss: 0.0025 - val_loss: 0.0051\n",
      "Epoch 188/200\n",
      "630/630 [==============================] - 0s - loss: 0.0025 - val_loss: 0.0052\n",
      "Epoch 189/200\n",
      "630/630 [==============================] - 0s - loss: 0.0025 - val_loss: 0.0051\n",
      "Epoch 190/200\n",
      "630/630 [==============================] - 0s - loss: 0.0025 - val_loss: 0.0052\n",
      "Epoch 191/200\n",
      "630/630 [==============================] - 0s - loss: 0.0025 - val_loss: 0.0052\n",
      "Epoch 192/200\n",
      "630/630 [==============================] - 0s - loss: 0.0025 - val_loss: 0.0051\n",
      "Epoch 193/200\n",
      "630/630 [==============================] - 0s - loss: 0.0024 - val_loss: 0.0051\n",
      "Epoch 194/200\n",
      "630/630 [==============================] - 0s - loss: 0.0025 - val_loss: 0.0051\n",
      "Epoch 195/200\n",
      "630/630 [==============================] - 0s - loss: 0.0025 - val_loss: 0.0051\n",
      "Epoch 196/200\n",
      "630/630 [==============================] - 0s - loss: 0.0025 - val_loss: 0.0051\n",
      "Epoch 197/200\n",
      "630/630 [==============================] - 0s - loss: 0.0024 - val_loss: 0.0050\n",
      "Epoch 198/200\n",
      "630/630 [==============================] - 0s - loss: 0.0024 - val_loss: 0.0051\n",
      "Epoch 199/200\n",
      "630/630 [==============================] - 0s - loss: 0.0024 - val_loss: 0.0051\n",
      "Epoch 200/200\n",
      "630/630 [==============================] - 0s - loss: 0.0023 - val_loss: 0.0051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f745828>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = Input(shape=(df.shape[1],))\n",
    "\n",
    "# encoding layers\n",
    "encoded = Dense(256, activation='relu')(input_data)\n",
    "encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(96, activation='relu')(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "# decoding layers\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(96, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(256, activation='relu')(decoded)\n",
    "decoded = Dense(df.shape[1], activation='sigmoid')(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_data, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_data, encoded)\n",
    "\n",
    "# compile the autoencoder\n",
    "autoencoder.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# and fit it!\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs = 200,\n",
    "                batch_size = 32,\n",
    "                shuffle = True,\n",
    "                validation_data = (X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Non-Zero Features: 23\n"
     ]
    }
   ],
   "source": [
    "encoded_data = encoder.predict(X_test)\n",
    "print('Number of Non-Zero Features:', sum(pd.DataFrame(encoded_data).sum() > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 121 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 211 / 211\n",
      "[t-SNE] Mean sigma: 6.870347\n",
      "[t-SNE] KL divergence after 75 iterations with early exaggeration: 0.369997\n",
      "[t-SNE] Error after 100 iterations: 0.369997\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npKb8f6N+5zusMmvQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12f5e14e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot t-SNE projection\n",
    "tsne = TSNE(verbose = 1,\n",
    "            n_components = 2,\n",
    "            perplexity = 40,\n",
    "            n_iter = 10000,\n",
    "            learning_rate = 200\n",
    "           )\n",
    "\n",
    "projection = tsne.fit_transform(encoded_data)\n",
    "plt.scatter(*projection.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Autoencoder Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all cols + pattern cols that have data\n",
    "non_zero_pattern_cols = (df[[col for col in df.columns.tolist() if '_pat_' in col]].sum() != 0)\n",
    "non_zero_pattern_cols = [col for col, is_non_zero in zip(pd.DataFrame(non_zero_pattern_cols).index.tolist(),\n",
    "                                                         pd.DataFrame(non_zero_pattern_cols)[0].tolist()\n",
    "                                                        ) if is_non_zero]\n",
    "\n",
    "cols = [col for col in df.columns.tolist() if '_pat_' not in col] + non_zero_pattern_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
